{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import wikipedia\n",
    "import spacy\n",
    "from textblob import TextBlob\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini Review Lab (50 Minutes)\n",
    "\n",
    "There are a lot of moving pieces in NLP and it is worthwhile to keep practicing the techniques we started to acquire yesterday. \n",
    "\n",
    "The first section of our lesson today will be a chance to review those topics and to practice discussing NLP and machine learning together. \n",
    "\n",
    "We'll be using a truncated version of the [Amazon Fine Food Review](https://www.kaggle.com/snap/amazon-fine-food-reviews/data) dataset. For a larger project, we would make use of the full set of data. However, in the interest of processing time, we'll use a randomly sampled set of 10,000 reviews for our training set and an additional 2,000 reviews for our test set.\n",
    "\n",
    "Your goal will be to create a predictive model that classifies a review into a high scoring review (5 stars) or not a high scoring review (1-4 stars). This value is already present in the data under the name `high_score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>high_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>177748</td>\n",
       "      <td>O. Brown \"Ms. O. Khannah-Brown\"</td>\n",
       "      <td>5</td>\n",
       "      <td>1190160000</td>\n",
       "      <td>Organic, Kosher, Tasty Assortment of Premium T...</td>\n",
       "      <td>*****&lt;br /&gt;Numi's Collection Assortment Melang...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>155411</td>\n",
       "      <td>Evon Schones \"ACMEFit\"</td>\n",
       "      <td>5</td>\n",
       "      <td>1252454400</td>\n",
       "      <td>YUM!!</td>\n",
       "      <td>I love this product!  We have replaced all oth...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>232693</td>\n",
       "      <td>Kerry Hart</td>\n",
       "      <td>4</td>\n",
       "      <td>1261699200</td>\n",
       "      <td>Good item for when I ate it....</td>\n",
       "      <td>I no longer eat kashi,,,too much sugar. But wh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>364352</td>\n",
       "      <td>Leonardo Rafael Camargo \"colombiaaaa\"</td>\n",
       "      <td>3</td>\n",
       "      <td>1214006400</td>\n",
       "      <td>an acquired taste, then it's good</td>\n",
       "      <td>it's not your typical pretzel. I don't eat muc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>491273</td>\n",
       "      <td>D. B. James</td>\n",
       "      <td>1</td>\n",
       "      <td>1348790400</td>\n",
       "      <td>WARNING:  High Fructose Corn Syrup</td>\n",
       "      <td>WARNING:  HIGH FRUCTOSE CORN SYRUP in this pro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id                            ProfileName  Score        Time  \\\n",
       "0  177748        O. Brown \"Ms. O. Khannah-Brown\"      5  1190160000   \n",
       "1  155411                 Evon Schones \"ACMEFit\"      5  1252454400   \n",
       "2  232693                             Kerry Hart      4  1261699200   \n",
       "3  364352  Leonardo Rafael Camargo \"colombiaaaa\"      3  1214006400   \n",
       "4  491273                            D. B. James      1  1348790400   \n",
       "\n",
       "                                             Summary  \\\n",
       "0  Organic, Kosher, Tasty Assortment of Premium T...   \n",
       "1                                              YUM!!   \n",
       "2                    Good item for when I ate it....   \n",
       "3                  an acquired taste, then it's good   \n",
       "4                 WARNING:  High Fructose Corn Syrup   \n",
       "\n",
       "                                                Text  high_score  \n",
       "0  *****<br />Numi's Collection Assortment Melang...           1  \n",
       "1  I love this product!  We have replaced all oth...           1  \n",
       "2  I no longer eat kashi,,,too much sugar. But wh...           0  \n",
       "3  it's not your typical pretzel. I don't eat muc...           0  \n",
       "4  WARNING:  HIGH FRUCTOSE CORN SYRUP in this pro...           0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('./datasets/amazon_train.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>high_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>202095</td>\n",
       "      <td>heather fox</td>\n",
       "      <td>5</td>\n",
       "      <td>1247097600</td>\n",
       "      <td>annies bunnies</td>\n",
       "      <td>great deal, these are so yummy , we love them ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>114978</td>\n",
       "      <td>kar</td>\n",
       "      <td>5</td>\n",
       "      <td>1328054400</td>\n",
       "      <td>Love it!!</td>\n",
       "      <td>I enjoy this snack and my son loves it too! It...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>287940</td>\n",
       "      <td>cherina hirsh</td>\n",
       "      <td>5</td>\n",
       "      <td>1281312000</td>\n",
       "      <td>Garlic always</td>\n",
       "      <td>This product is just fantastic for anyone who ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>67955</td>\n",
       "      <td>Lucy Dashwood \"Lucy Dashwood\"</td>\n",
       "      <td>5</td>\n",
       "      <td>1321574400</td>\n",
       "      <td>Delicious</td>\n",
       "      <td>The holidays wouldn't be festive without chest...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>454664</td>\n",
       "      <td>brenda peppers</td>\n",
       "      <td>5</td>\n",
       "      <td>1343088000</td>\n",
       "      <td>Toy Poodles Best Meal!</td>\n",
       "      <td>My little poddles love this brand and it's nic...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id                    ProfileName  Score        Time  \\\n",
       "0  202095                    heather fox      5  1247097600   \n",
       "1  114978                            kar      5  1328054400   \n",
       "2  287940                  cherina hirsh      5  1281312000   \n",
       "3   67955  Lucy Dashwood \"Lucy Dashwood\"      5  1321574400   \n",
       "4  454664                 brenda peppers      5  1343088000   \n",
       "\n",
       "                  Summary                                               Text  \\\n",
       "0          annies bunnies  great deal, these are so yummy , we love them ...   \n",
       "1               Love it!!  I enjoy this snack and my son loves it too! It...   \n",
       "2           Garlic always  This product is just fantastic for anyone who ...   \n",
       "3               Delicious  The holidays wouldn't be festive without chest...   \n",
       "4  Toy Poodles Best Meal!  My little poddles love this brand and it's nic...   \n",
       "\n",
       "   high_score  \n",
       "0           1  \n",
       "1           1  \n",
       "2           1  \n",
       "3           1  \n",
       "4           1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('./datasets/amazon_test.csv')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into pairs and work together to do the following.\n",
    "\n",
    "#### Model Generation (30 Minutes)\n",
    "\n",
    "1. Try and create a predictive model that identifies whether a review will be a high-scoring review or not (`high_score` feature in the data). While you can use any of the NLP techniques we discussed yesterday, here are some areas to focus on:\n",
    "\n",
    "1. Should you use `CountVectorizer` or `TfidfVectorizer` to transform your DataFrame?\n",
    "    - Keep stop words or drop them?\n",
    "    - Limit the words going in using `max_df` or `min_df`?\n",
    "2. Apply dimensionality reduction using `TruncatedSVD` or not?\n",
    "    - If you do, how many components should you keep?\n",
    "3. What modeling technique should you use? (`LogisticRegression`, `RandomForestClassifier`, etc.?) How will you change the hyperparameters.\n",
    "\n",
    "Make sure that you are checking your model's performance against the test set.\n",
    "\n",
    "#### Discussion (10 Minutes)\n",
    "\n",
    "A pair from each market will come on mic and discuss how they've chosen to transform their data. Additionally, we'll compare the **mean accuracy** for each market to see who has (at this point) made the most predictive model.\n",
    "\n",
    "#### Model Refinement (10 Minutes)\n",
    "\n",
    "Continue to refine your model or include some choices made by other markets. At the end of these 10 minutes, we'll report each market's best finding (and final model) by mic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(stop_words='english', max_features=1500)\n",
    "tfidf.fit(train['Text'])\n",
    "X_train = tfidf.transform(train['Text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=train['high_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Techniques\n",
    "\n",
    "Today's lesson is designed as an introduction to more advanced libraries or techniques in the realm of Natural Language Processing. These techniques can help you gain even greater accuracy in your modeling, but require more in-depth knowledge of new libraries, new techniques, etc. \n",
    "\n",
    "While we'll be introducing a lot of new material today, we'll be doing our best to limit the discussion to what is most immediately helpful. Each of these libraries and techniques has much more going on than we have time to discuss this week and we encourage you to spend time investigating and understanding these libraries. However, **mastery of these libraries, techniques, and materials introduced today is not required nor expected.**\n",
    "\n",
    "For Project 4 and your Capstone Project, if you are pursuing an NLP approach, these libraries may be very helpful. However, you can get a lot of mileage out of refining and using the sklearn libraries that we discussed yesterday. A good workflow is to try simple answers first and move into more advanced techniques as your use-case requires -- your goals as modelers should be to make best choice that you can, contingent on time and use-case. Having something work, but not be 100% correct is better than having something 100% correct that doesn't work yet. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `spacy` to extract parts of speech and named entities\n",
    "\n",
    "[`spaCy`](https://spacy.io/) is a large-scale NLP and text processing library designed to help you extract useful information from text in a speedy and accurate manner. You can imagine it like `CountVectorizer()` turned up to 11. It has underpinnings to C to increase speed and a focus on usability.\n",
    "\n",
    "`spaCy` does *so* much more than we are able to discuss at this point. It is quickly becoming the go-to library for text processing and feature extraction for text. Today, we'll use it to extract parts of speech and named entities.\n",
    "\n",
    "### Parts of Speech\n",
    "\n",
    "We may want to use some derived statistics about parts of speech in our work as Data Scientists, either as the inputs to a model (document _x_ is _y_% verbs) or to help us modify the inputs to a model (we may want to treat `book` the verb differently than `book` the noun). While many different libraries can do parts of speech (`textblob`, which we'll introduce shortly, can do that as well), we'll introduce this using `spaCy`.\n",
    "\n",
    "First, we set up some text from Wikipedia to parse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chicago ( ( listen) or ), officially the City of Chicago, is the third-most populous city in the United States. With over 2.7 million residents, it is also the most populous city in both the state of Illinois and the Midwestern United States. It is the county seat of Cook County. The Chicago metropolitan area, often referred to as Chicagoland, has nearly 10 million people and is the third-largest in the U.S. Chicago has often been called a global architecture capital. Chicago is considered one o\n"
     ]
    }
   ],
   "source": [
    "chicago = wikipedia.page('chicago')\n",
    "\n",
    "print(chicago.content[0:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's create sentences by splitting on `.`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chicago ( ( listen) or ), officially the City of Chicago, is the third-most populous city in the United States',\n",
       " 'With over 2.7 million residents, it is also the most populous city in both the state of Illinois and the Midwestern United States',\n",
       " 'It is the county seat of Cook County',\n",
       " 'The Chicago metropolitan area, often referred to as Chicagoland, has nearly 10 million people and is the third-largest in the U.S',\n",
       " 'Chicago has often been called a global architecture capital']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chicago_sents = chicago.content.split('. ')\n",
    "chicago_sents[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll set up a model in `spaCy`. This lets `spaCy` know what to use as its internal corpus. We name this model `nlp` by default:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we'll feed a sentence into `nlp`. This will automatically split the text into a generator of tokens (one token to each word). These tokens will have the part of speech already tagged in it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With over 2.7 million residents, it is also the most populous city in both the state of Illinois and the Midwestern United States\n",
      "With ADP\n",
      "over ADP\n",
      "2.7 NUM\n",
      "million NUM\n",
      "residents NOUN\n",
      ", PUNCT\n",
      "it PRON\n",
      "is VERB\n",
      "also ADV\n",
      "the DET\n",
      "most ADV\n",
      "populous ADJ\n",
      "city NOUN\n",
      "in ADP\n",
      "both ADJ\n",
      "the DET\n",
      "state NOUN\n",
      "of ADP\n",
      "Illinois PROPN\n",
      "and CCONJ\n",
      "the DET\n",
      "Midwestern PROPN\n",
      "United PROPN\n",
      "States PROPN\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(chicago_sents[1])\n",
    "print(doc)\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to convert this into a set of part of speech tags, we could add in a little extra Python to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NOUN': 3, 'PRON': 1, 'DET': 3, 'NUM': 2, 'ADJ': 2, 'VERB': 1, 'PROPN': 4, 'CCONJ': 1, 'PUNCT': 1, 'ADV': 2, 'ADP': 4}\n"
     ]
    }
   ],
   "source": [
    "tags = {}\n",
    "for token in doc:\n",
    "    if token.pos_ not in tags.keys():\n",
    "        tags[token.pos_] = 1\n",
    "    else:\n",
    "        tags[token.pos_] += 1\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many more tags to that `spaCy` can provide for us:\n",
    "\n",
    "- Text: The original word text.\n",
    "- Lemma: The base form of the word.\n",
    "- POS: The simple part-of-speech tag.\n",
    "- Tag: The detailed part-of-speech tag.\n",
    "- Dep: Syntactic dependency, i.e. the relation between tokens.\n",
    "- Shape: The word shape – capitalisation, punctuation, digits.\n",
    "- is alpha: Is the token an alpha character?\n",
    "- is stop: Is the token part of a stop list, i.e. the most common words of the language?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text\tLemma\tPOS\tDetailed POS\tDependency\tShape\tIs alphabetic?\tIs stopword?\n",
      "With\twith\tADP\tIN\tprep\tXxxx\tTrue\tTrue\n",
      "over\tover\tADP\tIN\tquantmod\txxxx\tTrue\tTrue\n",
      "2.7\t2.7\tNUM\tCD\tcompound\td.d\tFalse\tFalse\n",
      "million\tmillion\tNUM\tCD\tnummod\txxxx\tTrue\tFalse\n",
      "residents\tresident\tNOUN\tNNS\tpobj\txxxx\tTrue\tFalse\n",
      ",\t,\tPUNCT\t,\tpunct\t,\tFalse\tFalse\n",
      "it\t-PRON-\tPRON\tPRP\tnsubj\txx\tTrue\tTrue\n",
      "is\tbe\tVERB\tVBZ\tROOT\txx\tTrue\tTrue\n",
      "also\talso\tADV\tRB\tadvmod\txxxx\tTrue\tTrue\n",
      "the\tthe\tDET\tDT\tdet\txxx\tTrue\tTrue\n",
      "most\tmost\tADV\tRBS\tadvmod\txxxx\tTrue\tTrue\n",
      "populous\tpopulous\tADJ\tJJ\tamod\txxxx\tTrue\tFalse\n",
      "city\tcity\tNOUN\tNN\tattr\txxxx\tTrue\tFalse\n",
      "in\tin\tADP\tIN\tprep\txx\tTrue\tTrue\n",
      "both\tboth\tADJ\tPDT\tpobj\txxxx\tTrue\tTrue\n",
      "the\tthe\tDET\tDT\tdet\txxx\tTrue\tTrue\n",
      "state\tstate\tNOUN\tNN\tappos\txxxx\tTrue\tFalse\n",
      "of\tof\tADP\tIN\tprep\txx\tTrue\tTrue\n",
      "Illinois\tillinois\tPROPN\tNNP\tpobj\tXxxxx\tTrue\tFalse\n",
      "and\tand\tCCONJ\tCC\tcc\txxx\tTrue\tTrue\n",
      "the\tthe\tDET\tDT\tdet\txxx\tTrue\tTrue\n",
      "Midwestern\tmidwestern\tPROPN\tNNP\tcompound\tXxxxx\tTrue\tFalse\n",
      "United\tunited\tPROPN\tNNP\tcompound\tXxxxx\tTrue\tFalse\n",
      "States\tstates\tPROPN\tNNP\tconj\tXxxxx\tTrue\tFalse\n"
     ]
    }
   ],
   "source": [
    "print('\\t'.join(['Text', 'Lemma', 'POS', 'Detailed POS', 'Dependency',\n",
    "                'Shape', 'Is alphabetic?', 'Is stopword?']))\n",
    "for token in doc:\n",
    "    print('\\t'.join([token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "          token.shape_, str(token.is_alpha), str(token.is_stop)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check For Understanding 1 (10 minutes)\n",
    "\n",
    "With a partner, do the following:\n",
    "\n",
    "1. Pick two different wikipedia articles\n",
    "2. Get the content using the `wikipedia` library\n",
    "3. Using `spacy`, derive the following:\n",
    "    1. How many tokens are in your article?\n",
    "    2. How many parts of speech are in each article? How often do they occur?\n",
    "    3. As a percentage of the total number of tokens, how often does each part of speech occur?\n",
    "4. Does it look like there's a difference across your documents? What other types of documents would have different distributions of parts of speech?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entities\n",
    "\n",
    "Named entities are business, people, countries, or other things that refer to a specific person, place, or thing (think `Apple`, computer manufacturer versus `apple`, delicious crunchy fruit). `spaCy` can identify named entities for us, which we can either highlight or drop from our analyses. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've parsed a string of text using `spaCy`, we can call out the named entities using the `.ents` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc, type(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for named_entity in doc.ents:\n",
    "    print(named_entity.text, named_entity.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`spaCy` provides a set of labels for each type of named entity:\n",
    "\n",
    "|Label|Description|\n",
    "|:-- | :-- |\n",
    "|PERSON |\tPeople, including fictional. |\n",
    "|NORP |\tNationalities or religious or political groups. |\n",
    "|FACILITY |\tBuildings, airports, highways, bridges, etc. |\n",
    "|ORG |\tCompanies, agencies, institutions, etc. |\n",
    "|GPE |\tCountries, cities, states. |\n",
    "|LOC |\tNon-GPE locations, mountain ranges, bodies of water. |\n",
    "|PRODUCT |\tObjects, vehicles, foods, etc. (Not services.) |\n",
    "|EVENT |\tNamed hurricanes, battles, wars, sports events, etc. |\n",
    "|WORK_OF_ART |\tTitles of books, songs, etc. |\n",
    "|LAW |\tNamed documents made into laws. |\n",
    "|LANGUAGE |\tAny named language.|\n",
    "|DATE |\tAbsolute or relative dates or periods. |\n",
    "|TIME |\tTimes smaller than a day. |\n",
    "|PERCENT |\tPercentage, including \"%\".\n",
    "|MONEY |\tMonetary values, including unit. |\n",
    "|QUANTITY |\tMeasurements, as of weight or distance. |\n",
    "|ORDINAL |\t\"first\", \"second\", etc. |\n",
    "|CARDINAL |\tNumerals that do not fall under another type. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to see all the unique named entities in the Chicago page, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chicago.content[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NFL', '29% since', 'Columbian Exposition', 'the National Mall', 'the 1980s', 'September 10, 2012', '100 acres', 'Stanley', 'Millions', 'Miegs Field', 'NPR', 'Rate Field', 'Democrats', 'MacCormac College', 'the Kansas City Southern Railway', 'Each year', 'NYMEX', 'CBS', 'the Performing Arts Oriental Theatre', 'Four', 'Inland Northern American English', '43', '(800,000 barrels', '1997', '3 million', 'Renowned Chicago', 'the University of Chicago Cultural Policy Center', 'the Home Insurance Building', 'Michelin Guide 3 Star Award', 'Notable', 'McDonald', 'American College of Healthcare Executives', 'Martin Luther King', 'Seoul', '2016 World', '1939', '54 million', 'North', 'Rockford', 'Dow Jones Indexes', 'Kane', 'the Mississippi River', 'Orlando', 'first', 'The Blues Brothers', '(Eisenhower', 'British', 'the Miami-Illinois', 'Richard J. Daley', 'The University of Chicago Oriental Institute', 'the Northwestern Wildcats', '1905', 'Home Alone', 'The Chicago Police Department', 'John Hancock Center', 'North America', 'Charles B. Atwood', 'World', 'Croatians', 'CSO', '80', 'Cook County', 'the Saturday Night', 'the City of Chicago', 'City Colleges of Chicago', 'the Museum Campus', '1.308 million', 'American', 'Kearney', 'The University of Illinois College of Medicine', 'January 20, 1985', '\\n5.5% Asian', '1873', 'Two', 'Ottawa', 'Western Athletic Conference', '1979', 'AM', 'McCormick Place', '24‑hour', 'the Moody Bible Institute', 'DW60 Ferris', \"Dion O'Banion\", '1910', 'Armour Square', '10.4', 'American Community Survey', 'the Adler School of Professional Psychology', 'William Butler Yeats', 'Montenegrins', 'the spring', 'Burnham', 'across 26 miles', 'Robert Lostutter', 'Poles', 'Robert Morris University Illinois', 'early 1920s', 'The White', 'Dfa', 'Natural', 'Chicago Rockford International Airport', 'Labor', 'Washington', 'GE Transportation', 'Helmut Jahn', 'the Tribune Media', 'annual African American', 'Pizzeria Uno', '29,000 square meters', 'more than one', '\\n0.5% American', 'Native American', 'Divvy', 'Accreditation Council for Graduate Medical Education', '\\n\\nChicago Wilderness', '4,000', '47,074', 'August 12, 1833', '1940 to 1979', 'Machine', 'the mid-nineteenth century', 'the Great Lakes Megalopolis', '31.7% non', 'Queen of Peace High School', '11.09 million', '2,695,598', 'The Frugal Gourmet', 'National Basketball Association', 'the Lyric Opera of Chicago', 'Preston Bradley Hall', 'the Barack Obama Presidential Center', '2011', 'about 70', 'Jackson Parks', 'Checagou', \"the U.S. Census Bureau's\", 'Calumet', '1,000,000', 'Los Angeles', 'Millennium Park', 'the Northwest', 'South Shore Line', 'seventh', 'the Great Depression', 'six', \"Humboldt Park's\", 'StreetWise', 'the New York Mercantile Exchange', 'the Federal ATF', 'The Roman Catholic Archdiocese of Chicago', 'between 1910 and 1920', 'Michael Jordan', 'the North Side', 'about 75% of', 'Democratic Party', 'American Dental Association', 'DW60', 'San Antonio', 'Democrat', 'January', 'the century', \"Wacław Szymanowski's\", '15.94', 'Harry Caray', 'Transport Efficiency', 'Little Calumet River', 'Dan Ryan', 'more than 77% were', '14 million bushels', 'the summer of 2016', 'American Literature', '200', 'the 1940s', 'Uptown', 'the Harris Theater', 'Red', '1 mile', 'Yellow Cab Companies', 'Serbs', 'the last', \"Ferris Bueller's\", \"the Illinois State's\", 'Advanced Placement', '1907', '910', '66', 'seven', 'Douglas', 'the National Football League', '19th century', 'the Schwinn Bicycle Company', 'Paseo Boricua', 'Lorado Taft', 'Showtime', '53', 'the Pritzker Military Library', 'Chodzinski', 'the year', 'Illinois State Board of', '0.3% Vietnamese', '1867', 'Harold Washington College', '3', 'Harpo Studios', 'Horizon League', 'Illinois State', 'Belmont Avenue', 'Tony Accardo', '\\n\\n\\n', 'the Chicago Stock Exchange', 'Poetry', \"Loyola University Chicago's\", 'Italian', 'the Society for Human Rights', 'the Chicago Police Department', 'Harvey College', 'Ogden', 'the Hubbard Street Dance Chicago', 'Mies', 'Time Out Chicago', 'Broadway', 'Mean Girls, Wanted, Batman Begins, The Dark Knight, Transformers: Dark of the Moon, Transformers: Age of Extinction, Transformers: The Last Knight, Divergent, Batman v Superman: Dawn of Justice, Sinister 2 and Suicide Squad', 'the Chicago Literary Renaissance', '1955', 'African', 'The Chicagoland Chamber of Commerce', '2002', '0.2% Japanese', '42,063', 'the Chicago Imagists', '2.5', 'nearly two-thirds', 'The Chicago Tribune', '1833', 'The Chicago Blackhawks of', 'the 1780s', 'the Midway Plaisance', 'Brookfield', 'Amtrak', 'CTA', '30,000', 'November', 'the Southwest Side', 'the \"Founder of Chicago\"', 'Irv Kupcinet', 'Chicago State University', 'Columbia College Chicago', 'Montgomery Ward', '1993', '3,000 linear feet', 'the 20th century', '\\n32.9% Black', 'African-American', '1927', 'Kościuszko', 'The first', 'eight', 'Crate & Barrel', 'Barbara Rossi', 'Fairbanks', 'Between 1910 and 1930', 'Walgreens', 'Haymarket', 'Robb Report', 'Century of Progress International Exposition Worlds Fair', '\\n\\nThe Port of', 'Plan', 'DuPage', '18th', '1906', 'the Humboldt Park', '92', '0.40', 'Chicago Dance Crash', 'La Villita', 'the British School of Chicago', 'Oldenburg', 'The Onion', '98.1% of', '5b', 'Watch Dogs and', 'the National Basketball Association', 'Jackson Park', 'the Illinois International Port District', 'July 24, 1934', '0.4% Korean', 'Cascia High School', 'Christians', 'The Chicago', 'LED', 'Beyond the Beltway with', '1920s', '1992', 'Chopin Park', '1917', 'the last two decades', 'Midwest', 'the West Coast', 'Horto', 'Best Sports City', 'MasterCard Worldwide Centers of Commerce Index', 'Oak Park', 'the summer', '7% identity', 'the American Hospital Association', 'South Bend', 'Megabus', '2010', 'New York', 'Blue Island', 'Abbott Laboratories', 'Bosnians', 'Loop', 'CareerBuilder', 'approximately 196', 'the Daily Herald', 'Stan Mikita', 'Boeing', 'the Chicago Sister', 'the last half of the 19th century', 'WGN', 'Anish Kapoor', '490', 'seven years', 'Blackhawks', 'Seven', 'The Chicago Board of', 'Thanksgiving', 'MLS', 'Moose', 'Arts', 'Irish', '\\n\\nThe Regional Transportation Authority', '762', '2015', 'Sauk', 'Checker', 'the Arizona Cardinals', 'Greyhound Lines', '0.3% Colombian', 'three', 'West Town', '1872', 'Lake', 'the American Geographical Society Library\\nHistoric American Landscapes Survey', 'the mid-18th century', 'National Blue Ribbon School', 'ImprovOlympic', 'John H. Rauch', 'U.S. Department of Transportation', 'Taylor Street', '2007', 'the American League', 'Hyde Park', 'the University of Chicago', 'the Encyclopedia of Chicago', 'Exelon', 'the first half of 2013', 'Potawatomi', '1930', 'May 16, 2011', 'eleven', 'Detroit', 'Buddhists', 'the Chicago', '60', 'about 2 days', 'the Chicago Regional Port District', 'four', 'July 2016', 'Josephinum Academy', 'Major League Soccer', 'Brewster', '18.5', 'Calumet Harbor', 'the Driehaus Museum', 'Iowa', 'the late 1920s', '1,045,560', '3,000', 'twenty-six', 'four consecutive years form 2013 to 2016', 'Lutheran', 'twenty-four', 'Democratic National Convention', 'Rohe', 'Toyota Park', 'First', '38-foot (', '16.5', 'Milwaukee', '\\n45.0%', '\\n2.7% from', 'Bank of America Theatre', \"Edward Kemys's\", '943', 'CREATE', 'December 2, 1942', 'the Brookfield Zoo', 'Moore', 'the Chicago Freedom Movement', 'Leon Golub', 'the forty years', '250 million US gallons', 'COMEX', 'Miró', 'Mother McAuley Liberal Arts High School', '1874', 'Water Tower Place', '672', 'the Constitution', '24 hours', 'Resurrection High School', '1869', 'Garfield Park Conservatory', 'extends 60 miles', '780,000 square meters', 'Rick Bayless', 'Foreign trade zone', 'The Chicago Lincoln', 'New York City', 'Brown, Purple', '96', 'Crusader', 'Sears', 'More than half', '20th and 21st centuries', 'Wood Street', 'Law', 'March 4, 1837', 'today', '400,545', 'WMVP', 'between 1851 and 1920', 'Ida Crown', 'William Thompson', 'approximately 153,000', 'the 1870s and 1880s', '100,000', 'the Flag of Chicago', 'The Illinois Medical District', 'Time', 'West Side Chicago', 'the Commodities Exchange Inc.', 'Wrigley Field', '1998', 'Albanians', 'Rogers Park', 'the Catholic Theological Union', 'Joliet Junior College', 'Cosmos', 'the Harris Theater for Music and Dance', '97 km', 'three days', 'Richard Teller Crane', 'the Kinzie Street Bridge', 'Prison Break', 'summer', 'Pontiac', 'the Goodman Theatre', '15.65', '0.6% Ecuadorian', '1893', 'the Chicago Medical School', 'Bugmobile', 'Grand Calumet River', '1980', '1,200 acres', 'Christopher Columbus', 'fifty', 'Jefferson Park', '0.7% Guatemalan,', 'Ireland', '458', 'the Chicago Architecture Foundation', 'Southern,', 'Catholic', \"Prentice Women's\", 'the Great Northern Migration (Saar)', '13.2', '449', 'all three', '\"National Universities\"', 'ULTA Beauty', 'Morgan Park Academy', '1977', 'more than 4,000', 'five', 'the Calumet River', '32.9% in', 'as many as 21', 'the Lutheran School of Theology', 'Kraft Heinz', 'Five', 'WBBM', 'about 300', 'Chicagoland', '1.2% to 431 (', 'The Oprah Winfrey Show', '20', 'thousands', '42', 'Barack Obama', 'The Loop', 'the Shedd Aquarium', '\\n\\nSporting News named Chicago', 'Health & Society', 'the Buehler Center', 'Grant Park Music Festival', 'two-thirds', 'John Root', 'the Northern District', \"O'Hare\", 'West Loop', 'ComEd', 'Global Cities Index', 'Tom Memorial Park', 'PRI', 'the Southern United States', 'John Ashbery', 'Ace Hardware', 'the 1920s and 1930s', 'The Chicago Fire Soccer Club', 'Upscale', 'Northern American', 'the Chicago Reader', 'The City of Chicago', '45', 'Ogden Avenue', 'Daily News', 'February 1856', 'World War II', 'South Side', \"Frank Gehry's\", 'about $640 billion', 'the Museum of Broadcast Communications', 'Sears Tower', 'the Chicago Tribune', 'Michigan Avenue', 'nearly 25,000', 'Museum Campus', 'Dow 30 company', 'The McLaughlin Group', 'Jane Byrne', 'Frank Lloyd Wright', \"Frédéric Chopin's\", 'Andersonville', 'the Lycée Français de Chicago', '== Infrastructure', 'number one', 'the Chicago Defender', 'Auditorium Building', 'Feedburner', 'HALS', 'Magdalena Abakanowicz', 'Lollapalooza', 'Staff', 'Portland', 'Grant Achatz', '1883', 'Little Italy', 'Presbyterian', '416', '20 million', 'Langston Hughes', 'Enrico Fermi', 'Florida', 'Trump International Hotel', 'Boystown', 'two', 'Meštrović', 'Jones College Prep', 'Carmel High School', 'DePaul University', 'the Chicago Opera Theater and', '109', '22% have', '500', 'Kendall', '2013', '1 million', 'the Chicago Mercantile Exchange', 'Albert Raby', \"Lamb Chop's\", 'March 1937', '294', 'Lake Shore Drive', '94', 'Chagall', '44,103', 'the end of 2018', '4,331', '578', 'Blue Shield Association', 'the 1850s and 1860s', 'American College of Surgeons', '90s', 'Merc', 'Jaume Plensa', 'Boyle', 'Blue Cross', 'extends 10 feet', 'Democratic', '2009', 'Miro', 'about one', 'Las Vegas', 'The City Council', 'Agora', 'Amrany', 'more than 570', 'Wabash Avenue Bridge', '21.4% Mexican', 'FIFA', 'recent years', 'Iroquois County', 'June 15, 1835', 'Soldier Field', 'over 200', 'Art Institute of Chicago', 'Chicagoua', 'Lake Calumet', 'annually', 'nearly 150 percent', '28', 'One', 'the Chicago Cubs', 'Harriet Monroe', 'Desi', 'Greek', 'Fortune Global 500', \"Anish Kapoor's\", 'Lincoln', '28.9%.', 'the Hyde Park Township', 'Claes Oldenburg', 'Port Huron', 'the Museum of Science and Industry', '1687', '\\n28.9% Hispanic', '201 meters', 'Porter', 'Royal Baths', 'Black Belt', 'The Chicago Bulls of', '52', 'The North Side', 'United Continental Holdings', 'World War I', '27.5 million', 'Landfill', '1933', 'Burnham Park', 'the Illinois Institute of Art – Chicago', 'Kenwood', 'Iroquois Landing Lakefront Terminal', 'The Cook County', 'Baxter International', 'Dutch Wheels', '579 ft (', 'Italians', 'Western Avenue', '1885', 'Clark Street', '75.8', '29.3% of', 'The Illinois International Port', 'Vesuvio', 'Super Bowl XX', 'Sixteen Candles', 'Greektown', '3.8% Puerto', 'The River North Gallery District', 'Victory Gardens Theater', 'fifth', 'Derrick Rose', 'Jane Addams', '8', '2010–11 season', 'Navy', 'South Side Chicago', 'Cella', '0.1% Thai)', 'World Marathon Majors', 'the Feinberg School of Medicine', 'M.D.. Rauch', 'Carbondale', 'the Federal Reserve Bank of Chicago', 'Metra', 'Jens Ludwig', 'the United States of', 'Superfans', '2008', 'the United Center', 'William Rainey Harper', 'the Standard Oil Building', 'one', '6 miles', 'U.S. News & World Report', '2016 World Series', 'The Good Wife', 'Cook', 'Julius Rosenwald', '2014–2016', '2001', 'Discovery Channel', 'World Cities Research Network', 'more than US$13.7 billion', '79', 'Walter Payton', 'Blue', '1860', 'the White', 'Six', '== Education', 'Polish', '$2.5 billion', 'the Kansas–Nebraska Act', 'Northwestern University', 'Lincoln Park Conservatory', 'East', '1688', 'McKenna', 'NBC', 'Peoria', 'Commonwealth Edison', 'Saint Xavier University', 'Draugas', '59% in', 'about 130', 'Bowman', 'United Airlines', 'the Chicago River', 'the Polish Museum of America', 'fewer than 200', 'Batcolumn', 'Rice', 'Streeterville', 'about 29', 'The Willis Tower', 'WGN America', 'Wisconsin', 'NowSecure', 'Near North Side', '0.3% Cuban', 'the Seventh District', 'sixty years', 'the Rehabilitation Institute of Chicago', 'Francis W. Parker School', 'Parks', \"Los Angeles as America's\", 'Pilsen', 'the year 2016', 'Northeastern Illinois University', '4.0', 'GE Healthcare', 'the Chicago State Cougars', 'Chicago', '1953-54', 'The Chicago Loop', 'Frédéric Chopin', '1983', 'Joseph Jefferson Awards', 'Republican', 'The Bob Newhart Show', 'the Great Chicago Fire', 'Saint-Gaudens', 'Franklin D. Roosevelt', 'the U.S. Department of Education', 'Ten years later', 'Chicago Academy', 'Taste of Chicago festival', 'May 4, 1886', 'the Green Bay Packers', '176.5', 'the Chicago Fire Department', 'The Chicago Marathon', 'Sable', '1974', 'Rush University Medical Center', 'NHL', 'John Marshall', 'Plensa', \"Saint-Gaudens's\", 'Warsaw', 'Grand Rapids', '750', '17 Financial', 'about 4.48 million workers', 'the Chicago Sanitary', 'Bulls', 'fourth', 'the Art Institute of Chicago', 'roughly 4% of', 'Pink', '58', 'the twentieth century', 'roughly 60% of', 'the Great Chicago Fire of 1871', '34', 'the Chicago City Council', 'Henri Joutel', 'Cook County Jail', '22.1% of', '2012', '452', 'John Whitfield Bunn', 'December 20, 2014', 'Whitney M. Young', 'The Chicago Symphony Orchestra', 'Louis Sullivan', 'the Fine Arts Building', 'Flamingo', 'the early 1960s', 'State Street', 'the Institute of Gerontology of Ukraine', 'Adlai Stevenson', '205', 'LaPorte', '1950', 'Jesse Brown', 'Art Nouveau', 'The American Medical Association', 'Eugene Sawyer', '1.1% Filipino,', 'about 3,500', 'Bridgeview', 'Chicago High School', '1848', 'Fort Dearborn', 'The South Side', 'Maywood', 'Downers Grove', 'the University of Illinois Medical Center', '\"The Love Song of J. Alfred Prufrock\"', '55th', 'Summers', 'the Chicago Region Environmental', 'Missouri Valley Conference', 'Muslims', 'second', 'Czechs', 'calendar year 2014', '1795', 'West Ridge', '1889', '1900', '0.2% Peruvian', 'five years', '1901', '10% down', 'Eastern Europe', 'The 2015 year-end', 'less than 25% of', 'Near West Side', 'the 19th century', 'Street on', '41', 'University of Illinois', '\\n13.4% from', 'Lincoln Park', 'Daniel Burnham', 'several decades', 'Chicagoans', 'the Federal Reserve', 'WGN-TV', 'Jean Baptiste Point', 'General Electric', 'John Paul II', 'Anton Cermak', 'AT&T Plaza', '1908', '1984', 'the 1880s and 1890s', '90% black', 'Hull House', 'more than 6,000', 'Big East Conference', 'Glencoe', 'North American', 'the beginning of', 'Hong Kong', '55 percent', '22', 'Ezra Pound', 'third', 'the Windy City', \"New York's\", 'Havliček', 'Beast', 'Kennedy', 'Mike', 'Republicans', '10,000', '11-line', 'Peoples Gas', 'June', 'Much', 'several consecutive days', 'About 18.3%', '2014', 'South', '=\\n\\n71% of', '1803', 'Lithuanian Chicagoans', \"St. Valentine's Day Massacre\", 'Willis', 'Cleveland', 'Rochester', 'four distinct seasons', 'Cadillac Palace Theatre', 'the Allstate Arena', '35', 'the Chicago Board Options Exchange', 'Buckingham Fountain', 'Ravinia Festival', '10,000 or more', 'Shikaakwa', 'WSCR', 'Chicago Board of Health', 'Union Station', 'North Side', 'GRAB', 'Northside College Preparatory High School', 'the Treaty of Greenville', 'the Chicago Board of', 'the Sinaloa Cartel', 'the Erikson Institute', '1837', '2008–2012', 'National Louis University', 'Gateway Theatre', 'the University of Chicago Crime', 'the Tribune Broadcasting-', '1945', 'the University of Chicago Divinity School', 'between 2010 and 2040', 'the Chicago School', '2010–11', 'the Chicago Cardinals', '1987', 'Republican National Convention', '1866', 'The Head of State', 'about 300,000', 'Indian', 'Maxwell Street Polish', 'night', '98', 'nearly 400 acres', 'the Willis Tower', '61.7', 'eight seasons', 'San Francisco', 'SouthtownStar', '2.7 million', '5,800', 'WFLD', 'Mother Teresa', 'Midway', 'Latino', 'The Robie House', '233,903', 'nearly 10 million', '9.7 km', 'Chicago International Airport', 'Rosemont', \"St. Adalbert's\", 'Tribune', 'Stephen Douglas', '32', 'Al Capone', '468', 'Rush University', 'The Museum Campus', 'Indiana', 'The Institute for Clinical Social Work', 'the 1990s', 'Lawrence Avenue', 'Lake Calumet Harbor', 'the Dziennik Związkowy', 'about 200', 'Chicago Cityscape, Chicago, Cook County', 'Bill Savage', 'Green, Orange', '1924', 'Illinoisan', 'Calder', '2020', '1956', 'the Windy City Times', 'Egyptian', 'Strachovský', 'the Black Belt', 'the 1910s', 'the Steppenwolf Theatre Company', '176.2', 'nine', 'Chicago Public Radio', 'African Americans', 'December 2016', 'Tower, Museum of Science and Industry', 'the Ford Center', 'the Adler Planetarium & Astronomy Museum', 'Healthcare', 'Two years later', 'Perfect Strangers and its spinoff Family Matters, Punky Brewster, Married... with Children, Kenan', 'Monument', '46', 'the Chicago School of', '50.17 million', 'the Cumulus Media-owned', '38.9', 'June 2016', 'Jay Pritzker Pavilion', \"Richard J. Daley's\", 'over 3.6 million', 'Illinois Institute of Technology', 'the Jefferson Township', '16 percent', 'Loyola University Chicago', '40 km', '90', 'West Sides', 'the Evangelical Covenant Church', '190', 'over 2.7 million residents', \"T. S. Eliot's\", '415', 'Cristo Rey Jesuit', 'Seattle', 'the 1850s', 'between 1955 and 1971', '55.7', 'Broadway In', 'CBS Radio-', '1995', '\\nList of fiction', 'daily', 'Crown Fountain', \"O'Hare Airport\", 'each year', '2013–2014 20th', '2', 'Charlie Trotter', '448', '7', 'NBA', 'the Feltre School', 'The Lithuanian Opera Company', '31.7% in', '1985', '397', 'Jim Nutt', 'Köppen', 'Prohibition', 'the Chicago Loop', 'DePaul Blue Demons', 'Electronic Dance Music', 'the Chicago History Museum', 'Richard M. Daley', '1970', '1995 to 2008', '1.1% Indian', 'Kankakee', 'CME Group', 'Michelle Obama', 'This Great Migration', '2,600', 'about 4 miles', 'UBS', 'between 1920 and 1930', '435', 'Edmund Dick Taylor', 'Kennedy–King College', 'the City Beautiful Movement', 'the Cook County Forest', 'The Chicago Transit Authority', 'February 23, 2011', 'Jews', '9th', 'Chase Bank', 'Cloud Gate', 'DePaul College Prep', 'Grace', 'Richard J. Daley College', 'ThyssenKrupp North America', 'the Obama Foundation', 'the University of Illinois', 'Walk Score', 'South Halsted Street', 'the DuSable Museum of African American History', 'Lithuanian', 'Common Council', '1965', 'ABC', '160', 'Groupon', '1994', 'National Register of Historic Places', 'Institute of Puerto Rican Arts', 'more than 100,000', 'Chicagou', 'Chicago Fire', 'Navy Pier', 'Mexican', 'half', 'Jesuit', 'the Chicago Park District', 'Marist High School', 'Alexander Calder', 'Imagist', '\\nChicago Public Schools', 'HQ', 'Midway International Airport', 'the Western Hemisphere', '32.6% of', 'one day', 'Chinatown', 'Swedes', 'the U.S. Army Corps of', 'Tiffany', 'the Midwestern United States', 'WLS', 'annual', 'Academy of Nutrition', 'Alinea', 'the National Weather Service', \"Northern Indiana Commuter Transportation District's\", 'America', 'Newcity', \"Benjamin Ferguson's\", 'Dietetics, American Association of Nurse Anesthetists', '26%)', 'Malcolm X College', 'Caterpillar Inc.', '1969', 'Abakanowicz', '1.72 million', 'Chicago Cultural Center', 'Newark', 'Early Edition', \"Bill Swerski's\", '47,408', '2006', 'the American South', 'I.O.', 'Harlem', \"Ann & Robert H. Lurie Children's\", '97', \"Claire's\", 'Cook County Circuit Court', '315,000 square feet', 'the early 20th century', '1,000,000 m3', 'Abraham Lincoln', 'American League', 'Sunday', 'Cubs', '1912', 'the Chicago Bears', 'League', 'Marshall Field', 'Caribbean', 'Du Sable', 'the National Museum of Mexican Art', 'Ellen Gates Starr', '1968', '1975', 'The Chicago Public Library', '2004', 'sixth', 'Marc Chagall', '16', 'Lane Technical College Prep High School', 'the National Hockey League', '93', 'the U.S. Futures Exchange', 'World Series', 'Baroque', '22nd', 'the South Side', 'The West Side', 'the Chicago Botanic Garden', 'William Carlos Williams', 'over 8,000 acres', 'Grundy', 'Harold Washington', 'the Chicago Metropolitan Area', 'North Park University', '860-880', '5', '0.2% Salvadoran', 'Midway Airport', 'Symphony Center', 'Academy of General Dentistry', '1983–1987', \"Oprah Winfrey's\", 'Park Zoo', '1812', 'Tower', 'French', \"O'Hare International Airport\", 'Boston', 'the 1840s', 'Bears', 'UIC', 'Combo', 'LGBT', '$13', 'The Near West Side', 'Albany Park', '1934', 'The University of Chicago', 'the Chicago Public Library', 'Latin', 'New York Electric Air Line', 'the Chicago Sun-Times', 'North Avenue', 'five 50,000', 'Hegewisch', 'the Loyola Ramblers', 'the Eastern United States', 'John H. Stroger', '1871', 'Sneak Previews', 'Choose Chicago', 'Dalai Lama', 'LocalWiki Local Chicago Wiki', 'more than 14% of', 'PBS', 'RTA', '16.14', '1966', '3.6 million', 'John Crerar', 'Armour', 'Vietnam', 'Wilbur Wright College', 'sixteen', '2,900', '290', 'Allium', '1.6% Chinese', 'South Shore', 'Midwestern', 'River North', '88', 'Miami', 'Washington Park', 'every day of the year', 'White House', '970', 'McHenry', 'the Chicago White Sox', 'Stritch School of Medicine', 'the Chicago Shakespeare Theater', 'Jean Dubuffet', 'U.S.', 'June 4, 1998', \"Dubuffet's\", 'Puerto Rican', '54,188', 'the Great Lakes region', '1926', 'the Cleveland Indians', 'Indianapolis', 'Jack Brickhouse', 'Bruce DuMont', 'Roosevelt University', 'Germans', '1892', \"Dallin's Signal of Peace\", '6th', 'AP', 'Spearman', '8,390,000 square feet', 'Puerto Rican-', 'Europe', '900 linear meters (', 'the American Civil War', 'the Aon Center', 'the United States', 'the end of the 19th century', 'Nuclear Energy', 'The Merchandise Mart', 'Brioschi', 'the Democratic Party', 'between 1958 and early 1962)', 'the McCormick Place Convention Center', 'Visitor Information Center', 'up to eight adults', '0.3% Pakistani', 'DuSable Park', 'Rick Tramonto', 'Ford Motor Company', 'John Farwell', 'North Chicago', 'The Fourth Presbyterian Church', 'Major League Baseball', 'Robert de LaSalle', 'St. Ignatius College Preparatory School', 'every year', '\"The Loop\"', 'the University of Chicago Medical Center', 'the Chicago Race Riot', 'Carl Sandburg', 'June 2017', 'Truman College', 'Urbs', 'Urbana', '65', '2003', 'US', 'Illinois', 'City in a Garden', 'Archer Daniels Midland', 'the Chicago Public Schools', 'World Trade Center', 'Northerly Island', 'the National Register of Historic Places', 'Central Chicago', '2005', 'about $658.6 billion', 'the Soviet Union', 'The City Beautiful', '$1.95 billion', '4', 'U.S. House', 'Wrigley', 'Orbitz', 'early 2018', 'hundreds', '24.3', 'Saturday', '1929', 'Manhattan', '1919', '62.8% increase', '1679', '1:1‑scale', '77', \"Marshall Field's\", 'Lake Michigan', 'WYCC', 'The Midwestern University Chicago College of Osteopathic Medicine', 'tunneling two miles', 'United States', 'the Port District', 'Solidarity Promenade', 'Reagan', 'the Chicago Picasso', 'ten', 'Logan Square Boulevards Historic District', 'The Chicago School of Professional Psychology', 'nineteen boulevards', 'the Chicago Teachers Union', 'the Chicago Pride Parade', \"North America's\", 'Rahm Emanuel', \"the United States'\", 'Eternal Silence', 'Objectivist', '50', 'Central Park', 'the Chicago Portage', '200th', 'Condé Nast Traveler', 'Integrys Energy Group', 'Pace', 'Chase Tower', 'D.C.', 'early summer', 'the Latin School of Chicago', '44th', 'Heald Square Monument', 'since 1992', 'the Bulls (91', '19', '1850 to 1890', 'Lions', 'four years', 'up to 506', '26th Street', 'Gwendolyn Brooks', '2016', 'The A.V. Club', '1816', 'the winter season', 'Willis Tower', 'CBOE', 'The Joffrey Ballet', 'the Chicago Climate Exchange', 'Utilities', 'The Chicago Sky', 'Department of Transportation', 'Peggy Notebaert', 'the Evangelical Lutheran Church in America', 'Galena and Chicago Union Railroad', 'Ojibwe', 'Obama', \"The Alarm, Polasek's memorial to\", '580', 'Deerfield', 'American Osteopathic Association', '12.5% increase', 'Lithuanians', '55', '410', 'the Prairie School', 'Major League', 'Bennett', '2019', 'CBOT', 'Standing', 'September 9, 2013', '16th', 'Today', '6-mile (', 'the Chicago Building', 'The CME Group', 'The Illinois Department of Tourism', 'The Chicago-style', 'Thorvaldsen', '12', 'Chicago Festival Ballet', 'ESPN Radio-', '125', '0.2% Honduran', 'Hispanic', 'West University', 'Northwestern Memorial Hospital', 'several square miles', 'De La Salle Institute', 'James Merrill', 'Site Selection', '2005–2009', 'New Orleans', 'The Chicago Bears', 'weeks', 'Molly', 'Fox', 'Chicago Hope', 'Devon Avenue', 'July', 'the Brass Era', 'USDA', '600', 'Cabrini', '57', 'Shimer College', 'yearly', 'Bugs Moran', 'The Second City', 'Bronzeville', 'Roger Brown', 'The Metra Electric Line', 'Michigan Canal', '25 miles', 'Yellow', '29.7% of', 'Hindus', 'St. Louis', \"Puerto Rican People's Parade\", 'Gary', 'the Museum of Contemporary Art', 'Bobby Hull', 'winter', 'the Chicago Black Renaissance', 'three World Series', 'Quincy', 'ER', '1989', 'Northwest Side', '11', 'U.S. Receiver of Public Monies', 'Ivan Albright', '31', '42 km', '2006 WNBA season', \"O'Hare Airports\", 'the Pitchfork Music Festival', '75', '21st', 'Elston', 'Amrany and Rotblatt-Amrany', 'Copernicus', 'the School of the Art Institute of Chicago', 'Fountain', 'The Civic Opera House', 'the Field Museum of Natural History', 'the Sears Tower', 'Flying Dragon', '390', 'the National League', 'Ed Paschke', 'only one', 'Metaxa', 'Sufjan Stevens', 'the Illinois River', 'the Northwest Indian War', 'Division I', 'Grant Park', 'Americans', 'Accreditation Council for Continuing Medical Education', '105', 'the University of Chicago Laboratory Schools', 'North Side Chicago', '10-acre', 'the Polish Cathedral', 'the Chicago Board of Trade Building', 'the Great Lakes', 'Loop district', '1897', '1942', '1876', '−27 °F', 'the United States—', '510', 'Daley Plaza', 'African-Americans', 'Logan', 'Singapore', '355', 'American Society for Clinical Pathology', 'Crunelle', '5% decrease', 'seven years later'} 1528\n"
     ]
    }
   ],
   "source": [
    "chicago_model = nlp(chicago.content)\n",
    "named_entities = []\n",
    "for entity in chicago_model.ents:\n",
    "    named_entities.append(entity.text)\n",
    "print(set(named_entities), len(set(named_entities)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for Understanding 2 (10 Minutes)\n",
    "\n",
    "As a market (or as groups in your market), please discuss the following:\n",
    "\n",
    "1. As modelers, we will frequently have to make decisions about how to transform data. If you were using NLP to predict things, would it make sense to keep named entities? Would it make sense to drop them? If it would depend on the circumstances, under what circumstances would it make sense to keep or drop named entities?\n",
    "\n",
    "We'll have a couple of markets come on mic to discuss cases they identified where keeping named entities might make sense and cases where it would not make sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `textblob` to do sentiment analysis\n",
    "\n",
    "We can also use a library known as [`textblob`](https://textblob.readthedocs.io/en/dev/) to do a **lot** of text transformation and extraction on our behalf. For our purposes, we are going to use it to analyze text and derive the overall sentiment of the text.\n",
    "\n",
    "Sentiment can be split into two related scales:\n",
    "\n",
    "- subjectivity (0 to 1): scores closer to 0 are more objective in tone, scores closer to 1 are more subjective in tone\n",
    "- polarity (-1 to 1): scores closer to -1 are more negative in tone, closer to 0 are more neutral, and closer to 1 are more positive in tone.\n",
    "\n",
    "Using `textblob` is user-friendly -- pass a string into a `Textblob()` class and then call the `.sentiment.polarity` or `sentiment.subjectivity` attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "really_good_review = '''\n",
    "Goodness me, what a fantastic movie. \n",
    "Caught the world premiere at the Toronto International Film Festival and \n",
    "the entire theater laughed until they cried. \n",
    "Amazingly directed, HILARIOUSLY funny, it blends a 1930s gangster \n",
    "stylishness into a Hong Kong kung fu movie to astonishing results. \n",
    "Who would've thought you could top Shaolin Soccer? \n",
    "Not me, until I saw this movie. Stephen Chow pulled it off. \n",
    "Chow's comedic timing gets better and better with every movie \n",
    "he makes, and while his films are depending more and more on \n",
    "CGI these days, and makes this movie much more a fantasy kung \n",
    "fu film than a traditional one, it hardly detracts from the \n",
    "enjoyable experience. Make it your mission to see this film - \n",
    "it will be one of the most entertaining you ever see. \n",
    "I can't remember the last film I enjoyed myself in more. \n",
    "My eyes still hurt from wiping away tears of laughter. Seriously.  \n",
    "'''\n",
    "\n",
    "really_bad_review = '''\n",
    "Thank you for coming into your performance review Mr. Smith.\n",
    "The company is concerned about your performance. Lately your work has \n",
    "been subpar and at times counter to this company's stated goals.\n",
    "Your demeanor has been aggresive and at times hostile to your \n",
    "fellow coworkers.\n",
    "We have no choice but to terminate your employment, effective \n",
    "immediately. Thank you.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5749999999999998 0.33295454545454545\n",
      "0.7 0.15\n"
     ]
    }
   ],
   "source": [
    "good_review = TextBlob(really_good_review)\n",
    "print(good_review.sentiment.subjectivity, good_review.sentiment.polarity)\n",
    "\n",
    "bad_review = TextBlob(really_bad_review)\n",
    "print(bad_review.sentiment.subjectivity, bad_review.sentiment.polarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for Understanding 3 (5 Minutes)\n",
    "\n",
    "Individually, please answer the following:\n",
    "\n",
    "1. What type of subjectivity and polarity scores would you expect wikipedia articles to have?\n",
    "2. Confirm your hypothesis by using `textblob` on some of the wikipedia pages we have used so far. Were your thoughts confirmed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3412846858940613 0.10109983766233746\n"
     ]
    }
   ],
   "source": [
    "chi_wiki=TextBlob(chicago.content)\n",
    "print(chi_wiki.sentiment.subjectivity, chi_wiki.sentiment.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7 0.15\n"
     ]
    }
   ],
   "source": [
    "bad_chi_wiki = TextBlob(really_bad_review)\n",
    "print(bad_chi_wiki.sentiment.subjectivity, bad_chi_wiki.sentiment.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding these features to DataFrames\n",
    "\n",
    "We may want to include these features into a DataFrame for use in a later model. The most straightforward way to do so would be to apply them using Pandas.\n",
    "\n",
    "Here, we'll make use of the same dataset on economic news that we used yesterday. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "econ = pd.read_csv('datasets/economic_news.csv',\n",
    "                  usecols=[7, 11, 14],\n",
    "                  nrows=200)\n",
    "econ['text'] = econ['text'].apply(lambda x: x.replace('</br>', ''))\n",
    "econ['relevance'] = econ['relevance'].apply(lambda x: 1 if x == 'yes' else 0)\n",
    "econ.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(econ[['text']],\n",
    "                                                   econ['relevance'],\n",
    "                                                   test_size=0.50,\n",
    "                                                   random_state=8675309)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use `spaCy` to create a column for the number of monetary-based named entities, followed by using `textblob` to create a polarity score for each article.\n",
    "\n",
    "While we can try to put this into a lambda function, it will probably be easiest in this case to define four functions and apply them.\n",
    "\n",
    "However, because we're sequentially loading up each row of data and processing it, this can be a little bit of a time and memory sink. Expect processing to take some extra time for this step.*\n",
    "\n",
    "* **note**: for spacy, there are faster ways to process the data that do not involve pushing it through Pandas. Investigate the spacy `pipe` method if you're looking to do a larger amount of text transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_monetary_ents(text):\n",
    "    text = nlp(text)\n",
    "    return len([x.text for x in text.ents if x.label_ == 'MONEY'])\n",
    "\n",
    "def polarity(text):\n",
    "    text = TextBlob(text)\n",
    "    return text.sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "invalid index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/scipy/sparse/csr.py\u001b[0m in \u001b[0;36masindices\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0midx_dtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m                     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'text'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-19c14008510a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_monetary'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber_of_monetary_ents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/scipy/sparse/csr.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index_to_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m         \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masindices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m         \u001b[0mcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masindices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/scipy/sparse/csr.py\u001b[0m in \u001b[0;36masindices\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    244\u001b[0m                     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'invalid index'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: invalid index"
     ]
    }
   ],
   "source": [
    "X_train['num_monetary'] = X_train['text'].apply(number_of_monetary_ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "invalid index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/scipy/sparse/csr.py\u001b[0m in \u001b[0;36masindices\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0midx_dtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m                     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'num_monetary'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-672202dfb6f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_monetary'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mchicago\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/scipy/sparse/csr.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index_to_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m         \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masindices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m         \u001b[0mcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masindices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/scipy/sparse/csr.py\u001b[0m in \u001b[0;36masindices\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    244\u001b[0m                     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'invalid index'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: invalid index"
     ]
    }
   ],
   "source": [
    "X_train['num_monetary'].describe()\n",
    "chicago.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['num_monetary'].plot(kind='hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train['polarity'] = X_train['text'].apply(polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['polarity'].describe()\n",
    "X_train['polarity'].plot(kind='hist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also pass this into a predictive model to see if these features can assist predicting economic status:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(X_train[['num_monetary', 'polarity']], y_train)\n",
    "print(rfc.score(X_train[['num_monetary', 'polarity']], y_train))\n",
    "predictions = rfc.predict(X_train[['num_monetary', 'polarity']])\n",
    "print(confusion_matrix(y_train, predictions))\n",
    "print(classification_report(y_train, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test['num_monetary'] = X_test['text'].apply(number_of_monetary_ents)\n",
    "X_test['polarity'] = X_test['text'].apply(polarity)\n",
    "print(rfc.score(X_test[['num_monetary', 'polarity']], y_test))\n",
    "predictions = rfc.predict(X_test[['num_monetary', 'polarity']])\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: you're probably wondering _why_ we would try these things when they don't seem to immediately help. During a larger project, we will likely spend days if not weeks on feature extraction and analysis and will want to make as many useful features as possible to make as good a model as possible. Other techniques may involve more nuanced modeling, such as looking at the sequence of parts of speech, etc. Part of this lesson is designed to expose to what is out there so that when faced with a situation where those techniques may be useful, you're aware of their existence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assigning documents to topics using LDA\n",
    "\n",
    "LDA (Latent Dirichlet Allocation) is an unstructured machine learning technique that iteratively attempts to find clusters of words that are likely to happen together across multiple documents. We interpret the co-occurance of these words together to be analgous to different topics discussed in across a body of documents. \n",
    "\n",
    "LDA works by iteratively guessing how likely a given word is to be part of a given topic until we tell it to stop. \n",
    "\n",
    "This process of updating probabilities will make more sense after next weeks lectures on Bayes, but we'll quickly discuss here and move forward.\n",
    "\n",
    "(Explanation cribbed from [Introduction to Latent Dirichlet Allocation](http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/))\n",
    "\n",
    "We begin by picking a set of documents and a number of topics that we want to generate. One way that we do this is what's known as collapsed Gibbs saampling. We do the following:\n",
    "\n",
    "1. Randomly assign every word in every document to one of the $k$ topics:\n",
    "    - $w$: a word in a document\n",
    "    - $d$: a document\n",
    "    - $k$: a topic\n",
    "2. At this point, every word has a likelihood that they belong in a given a topic, based on the other words in documents that they exist in. \n",
    "3. Iterate through every word in every document and:\n",
    "    1. Assume that every other word has the correct likelihood that they belong to each topic (so, `apple` might have a distribution of `[0.1, 0.1, 0.2, 0.4, 0.2]` for five topics.\n",
    "    2. Look at the likelihood of seeing word $w$ in document $d$ and adjust the topic probabilities as needed\n",
    "    > for example, if there are a lot of words in topic 1 in document $d$ and word $w$ has a stronger likelihood of being in topic 2, because we're assuming that every **other** distribution is correct, we should change our understanding of where word $w$ belongs and tweak it more in favor of belonging to topic 1, not topic 2\n",
    "    \n",
    "You can kind of interpret this with an analogy:\n",
    "\n",
    "> Imagine you move to a new town and you don't know what sort of people you want to hang out with. You imagine there's five different groups of people. You start visiting different places around town (the park, the library, the mall, etc.) and noting who's there. Everytime you go to a place you start adjusting your expectation on who you'll see there (such as the goths constantly are at the mall, so we should expect less and less that they'll show up at the library). This is (very roughly) analgous to what LDA is doing.\n",
    "\n",
    "The name latent dirichlet allocation should begin to make more sense in this context:\n",
    "- latent -- because we have no explicit marker of topic and are grouping things together based on features we are inferring, not seeing\n",
    "- [dirichlet](https://en.wikipedia.org/wiki/Dirichlet_distribution) -- is a type of probability distribution for multiple vectors at once (like a bunch of words towards a bunch of topics)\n",
    "- allocation -- we are allocating different words to different topics via this iterative updating of priors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both sklearn and `gensim`, a library we will discuss in the context of a technique called `word2vec`, can handle LDA. However, we'll rely on the sklearn implementation here to reduce the amount of extra work we'll need to do in picking up a new library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's reimport all of the economic news data instead of just the first 200 rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "econ = pd.read_csv('datasets/economic_news.csv',\n",
    "                  usecols=[14])\n",
    "econ['text'] = econ['text'].apply(lambda x: x.replace('</br>', ''))\n",
    "econ.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll transform the data using `CountVectorizer` and removing stop words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words='english')\n",
    "cv.fit(econ['text'].values)\n",
    "X = cv.transform(econ['text'].values)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll instantiate an LDA and fit it to our sparse matrix of words. We have to provide a number of topics that we are looking for (in this case, we're looking for 5 topics). We'll also store the names of the each of the words created during the `CountVectorizer` step for use with the LDA results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = cv.get_feature_names()\n",
    "lda = LatentDirichletAllocation(n_components=5)\n",
    "\n",
    "lda.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of our work will be held in the `.components_` feature. Each row of this array is one of our topics and each column (in order) is a word created by `CountVectorizer`. The values are the relative \"likelihoods\" that the word $w$ should be in topic $t$.\n",
    "\n",
    "> From the sklearn docs, `.components_`: \"can be viewed as pseudocount that represents the number of times word j was assigned to topic i. It can also be viewed as distribution over the words for each topic after normalization\" (we could normalize by dividing row total for that topic). \n",
    "\n",
    "For our purposes, it's enough to say that bigger values means the word belongs more in that topic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lda.components_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = pd.DataFrame(lda.components_,\n",
    "                      columns=feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see what words are most likely in each topic, we could sort by the biggest values for each topic.\n",
    "\n",
    "> Every feature has a likelihood of being in a topic, just a very, very low one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic in range(5):\n",
    "    print('Topic', topic)\n",
    "    word_list = results.T[topic].sort_values(ascending=False).index\n",
    "    print(' '.join(word_list[0:25]), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we change the number of topics, we should see the topics change slightly. Remember that because this is an unstructured technique our editorial power as the modeler is important to identify useful topics. \n",
    "\n",
    "However, this provides a powerful tool to create summaries of larger bodies of documents!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for Understanding 4 (20 Minutes)\n",
    "\n",
    "In pairs, do the following:\n",
    "\n",
    "1. Rerun the LDA, choosing 10 topics instead of 5. \n",
    "> Make sure that you can explain what each line of the code does to each other. This can be as generic as \"This runs an LDA with 10 components on a matrix of words and documents\" but it's important to be able to explain what a block of code is doing. In particular, make sure that you're able to explain what has happened in this line of code above `word_list = results.T[topic].sort_values(ascending=False).index` -- if you need to, start with the very first portion (`results`) and investigate what each subsequent step does.\n",
    "2. Look at the results of your LDA. How would you summarize what each topic says?\n",
    "3. Does 10 look to be a correct number of topics? Are the same words showing up in multiple topics? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
