{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wikipedia'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c51596547aed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mwikipedia\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtextblob\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wikipedia'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import wikipedia\n",
    "import spacy\n",
    "from textblob import TextBlob\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini Review Lab (50 Minutes)\n",
    "\n",
    "There are a lot of moving pieces in NLP and it is worthwhile to keep practicing the techniques we started to acquire yesterday. \n",
    "\n",
    "The first section of our lesson today will be a chance to review those topics and to practice discussing NLP and machine learning together. \n",
    "\n",
    "We'll be using a truncated version of the [Amazon Fine Food Review](https://www.kaggle.com/snap/amazon-fine-food-reviews/data) dataset. For a larger project, we would make use of the full set of data. However, in the interest of processing time, we'll use a randomly sampled set of 10,000 reviews for our training set and an additional 2,000 reviews for our test set.\n",
    "\n",
    "Your goal will be to create a predictive model that classifies a review into a high scoring review (5 stars) or not a high scoring review (1-4 stars). This value is already present in the data under the name `high_score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>high_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>177748</td>\n",
       "      <td>O. Brown \"Ms. O. Khannah-Brown\"</td>\n",
       "      <td>5</td>\n",
       "      <td>1190160000</td>\n",
       "      <td>Organic, Kosher, Tasty Assortment of Premium T...</td>\n",
       "      <td>*****&lt;br /&gt;Numi's Collection Assortment Melang...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>155411</td>\n",
       "      <td>Evon Schones \"ACMEFit\"</td>\n",
       "      <td>5</td>\n",
       "      <td>1252454400</td>\n",
       "      <td>YUM!!</td>\n",
       "      <td>I love this product!  We have replaced all oth...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>232693</td>\n",
       "      <td>Kerry Hart</td>\n",
       "      <td>4</td>\n",
       "      <td>1261699200</td>\n",
       "      <td>Good item for when I ate it....</td>\n",
       "      <td>I no longer eat kashi,,,too much sugar. But wh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>364352</td>\n",
       "      <td>Leonardo Rafael Camargo \"colombiaaaa\"</td>\n",
       "      <td>3</td>\n",
       "      <td>1214006400</td>\n",
       "      <td>an acquired taste, then it's good</td>\n",
       "      <td>it's not your typical pretzel. I don't eat muc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>491273</td>\n",
       "      <td>D. B. James</td>\n",
       "      <td>1</td>\n",
       "      <td>1348790400</td>\n",
       "      <td>WARNING:  High Fructose Corn Syrup</td>\n",
       "      <td>WARNING:  HIGH FRUCTOSE CORN SYRUP in this pro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id                            ProfileName  Score        Time  \\\n",
       "0  177748        O. Brown \"Ms. O. Khannah-Brown\"      5  1190160000   \n",
       "1  155411                 Evon Schones \"ACMEFit\"      5  1252454400   \n",
       "2  232693                             Kerry Hart      4  1261699200   \n",
       "3  364352  Leonardo Rafael Camargo \"colombiaaaa\"      3  1214006400   \n",
       "4  491273                            D. B. James      1  1348790400   \n",
       "\n",
       "                                             Summary  \\\n",
       "0  Organic, Kosher, Tasty Assortment of Premium T...   \n",
       "1                                              YUM!!   \n",
       "2                    Good item for when I ate it....   \n",
       "3                  an acquired taste, then it's good   \n",
       "4                 WARNING:  High Fructose Corn Syrup   \n",
       "\n",
       "                                                Text  high_score  \n",
       "0  *****<br />Numi's Collection Assortment Melang...           1  \n",
       "1  I love this product!  We have replaced all oth...           1  \n",
       "2  I no longer eat kashi,,,too much sugar. But wh...           0  \n",
       "3  it's not your typical pretzel. I don't eat muc...           0  \n",
       "4  WARNING:  HIGH FRUCTOSE CORN SYRUP in this pro...           0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('./datasets/amazon_train.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>high_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>202095</td>\n",
       "      <td>heather fox</td>\n",
       "      <td>5</td>\n",
       "      <td>1247097600</td>\n",
       "      <td>annies bunnies</td>\n",
       "      <td>great deal, these are so yummy , we love them ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>114978</td>\n",
       "      <td>kar</td>\n",
       "      <td>5</td>\n",
       "      <td>1328054400</td>\n",
       "      <td>Love it!!</td>\n",
       "      <td>I enjoy this snack and my son loves it too! It...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>287940</td>\n",
       "      <td>cherina hirsh</td>\n",
       "      <td>5</td>\n",
       "      <td>1281312000</td>\n",
       "      <td>Garlic always</td>\n",
       "      <td>This product is just fantastic for anyone who ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>67955</td>\n",
       "      <td>Lucy Dashwood \"Lucy Dashwood\"</td>\n",
       "      <td>5</td>\n",
       "      <td>1321574400</td>\n",
       "      <td>Delicious</td>\n",
       "      <td>The holidays wouldn't be festive without chest...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>454664</td>\n",
       "      <td>brenda peppers</td>\n",
       "      <td>5</td>\n",
       "      <td>1343088000</td>\n",
       "      <td>Toy Poodles Best Meal!</td>\n",
       "      <td>My little poddles love this brand and it's nic...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id                    ProfileName  Score        Time  \\\n",
       "0  202095                    heather fox      5  1247097600   \n",
       "1  114978                            kar      5  1328054400   \n",
       "2  287940                  cherina hirsh      5  1281312000   \n",
       "3   67955  Lucy Dashwood \"Lucy Dashwood\"      5  1321574400   \n",
       "4  454664                 brenda peppers      5  1343088000   \n",
       "\n",
       "                  Summary                                               Text  \\\n",
       "0          annies bunnies  great deal, these are so yummy , we love them ...   \n",
       "1               Love it!!  I enjoy this snack and my son loves it too! It...   \n",
       "2           Garlic always  This product is just fantastic for anyone who ...   \n",
       "3               Delicious  The holidays wouldn't be festive without chest...   \n",
       "4  Toy Poodles Best Meal!  My little poddles love this brand and it's nic...   \n",
       "\n",
       "   high_score  \n",
       "0           1  \n",
       "1           1  \n",
       "2           1  \n",
       "3           1  \n",
       "4           1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('./datasets/amazon_test.csv')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into pairs and work together to do the following.\n",
    "\n",
    "#### Model Generation (30 Minutes)\n",
    "\n",
    "1. Try and create a predictive model that identifies whether a review will be a high-scoring review or not (`high_score` feature in the data). While you can use any of the NLP techniques we discussed yesterday, here are some areas to focus on:\n",
    "\n",
    "1. Should you use `CountVectorizer` or `TfidfVectorizer` to transform your DataFrame?\n",
    "    - Keep stop words or drop them?\n",
    "    - Limit the words going in using `max_df` or `min_df`?\n",
    "2. Apply dimensionality reduction using `TruncatedSVD` or not?\n",
    "    - If you do, how many components should you keep?\n",
    "3. What modeling technique should you use? (`LogisticRegression`, `RandomForestClassifier`, etc.?) How will you change the hyperparameters.\n",
    "\n",
    "Make sure that you are checking your model's performance against the test set.\n",
    "\n",
    "#### Discussion (10 Minutes)\n",
    "\n",
    "A pair from each market will come on mic and discuss how they've chosen to transform their data. Additionally, we'll compare the **mean accuracy** for each market to see who has (at this point) made the most predictive model.\n",
    "\n",
    "#### Model Refinement (10 Minutes)\n",
    "\n",
    "Continue to refine your model or include some choices made by other markets. At the end of these 10 minutes, we'll report each market's best finding (and final model) by mic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "On Training Data\n",
      "0.7401\n",
      "[[1106 2536]\n",
      " [  63 6295]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.30      0.46      3642\n",
      "          1       0.71      0.99      0.83      6358\n",
      "\n",
      "avg / total       0.80      0.74      0.69     10000\n",
      "\n",
      "\n",
      "On Test Data\n",
      "0.6895\n",
      "[[ 136  592]\n",
      " [  29 1243]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      0.19      0.30       728\n",
      "          1       0.68      0.98      0.80      1272\n",
      "\n",
      "avg / total       0.73      0.69      0.62      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample model code for this model -- not optimized!\n",
    "\n",
    "# Use tfidf to take out 1,500 of the most common features with stop words removed\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words='english', max_features=1500)\n",
    "tfidf.fit(train['Text'])\n",
    "X = tfidf.transform(train['Text'])\n",
    "\n",
    "# 1,500 features might be quite large, so use TruncatedSVD to reduce to 150 \n",
    "tsvd = TruncatedSVD(n_components=150)\n",
    "tsvd.fit(X)\n",
    "X = tsvd.transform(X)\n",
    "\n",
    "# Use a RandomForestClassifier to create the model\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=100, max_depth=7)\n",
    "rfc.fit(X, train['high_score'])\n",
    "\n",
    "# Results on training data\n",
    "\n",
    "print('\\nOn Training Data')\n",
    "print(rfc.score(X, train['high_score']))\n",
    "train_predictions = rfc.predict(X)\n",
    "\n",
    "print(confusion_matrix(train['high_score'], train_predictions))\n",
    "print(classification_report(train['high_score'], train_predictions))\n",
    "\n",
    "# Results on test data\n",
    "\n",
    "test_X = tfidf.transform(test['Text'])\n",
    "test_X = tsvd.transform(test_X)\n",
    "\n",
    "print('\\nOn Test Data')\n",
    "print(rfc.score(test_X, test['high_score']))\n",
    "test_predictions = rfc.predict(test_X)\n",
    "\n",
    "print(confusion_matrix(test['high_score'], test_predictions))\n",
    "print(classification_report(test['high_score'], test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Techniques\n",
    "\n",
    "Today's lesson is designed as an introduction to more advanced libraries or techniques in the realm of Natural Language Processing. These techniques can help you gain even greater accuracy in your modeling, but require more in-depth knowledge of new libraries, new techniques, etc. \n",
    "\n",
    "While we'll be introducing a lot of new material today, we'll be doing our best to limit the discussion to what is most immediately helpful. Each of these libraries and techniques has much more going on than we have time to discuss this week and we encourage you to spend time investigating and understanding these libraries. However, **mastery of these libraries, techniques, and materials introduced today is not required nor expected.**\n",
    "\n",
    "For Project 4 and your Capstone Project, if you are pursuing an NLP approach, these libraries may be very helpful. However, you can get a lot of mileage out of refining and using the sklearn libraries that we discussed yesterday. A good workflow is to try simple answers first and move into more advanced techniques as your use-case requires -- your goals as modelers should be to make best choice that you can, contingent on time and use-case. Having something work, but not be 100% correct is better than having something 100% correct that doesn't work yet. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `spacy` to extract parts of speech and named entities\n",
    "\n",
    "[`spaCy`](https://spacy.io/) is a large-scale NLP and text processing library designed to help you extract useful information from text in a speedy and accurate manner. You can imagine it like `CountVectorizer()` turned up to 11. It has underpinnings to C to increase speed and a focus on usability.\n",
    "\n",
    "`spaCy` does *so* much more than we are able to discuss at this point. It is quickly becoming the go-to library for text processing and feature extraction for text. Today, we'll use it to extract parts of speech and named entities.\n",
    "\n",
    "### Parts of Speech\n",
    "\n",
    "We may want to use some derived statistics about parts of speech in our work as Data Scientists, either as the inputs to a model (document _x_ is _y_% verbs) or to help us modify the inputs to a model (we may want to treat `book` the verb differently than `book` the noun). While many different libraries can do parts of speech (`textblob`, which we'll introduce shortly, can do that as well), we'll introduce this using `spaCy`.\n",
    "\n",
    "First, we set up some text from Wikipedia to parse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chicago ( ( listen) or ), officially the City of Chicago, is the third-most populous city in the United States. With over 2.7 million residents, it is also the most populous city in both the state of Illinois and the Midwestern United States. It is the county seat of Cook County. The Chicago metropolitan area, often referred to as Chicagoland, has nearly 10 million people and is the third-largest in the U.S. Chicago has often been called a global architecture capital. Chicago is considered one o\n"
     ]
    }
   ],
   "source": [
    "chicago = wikipedia.page('chicago')\n",
    "\n",
    "print(chicago.content[0:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's create sentences by splitting on `.`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chicago ( ( listen) or ), officially the City of Chicago, is the third-most populous city in the United States',\n",
       " 'With over 2.7 million residents, it is also the most populous city in both the state of Illinois and the Midwestern United States',\n",
       " 'It is the county seat of Cook County',\n",
       " 'The Chicago metropolitan area, often referred to as Chicagoland, has nearly 10 million people and is the third-largest in the U.S',\n",
       " 'Chicago has often been called a global architecture capital']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chicago_sents = chicago.content.split('. ')\n",
    "chicago_sents[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll set up a model in `spaCy`. This lets `spaCy` know what to use as its internal corpus. We name this model `nlp` by default:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we'll feed a sentence into `nlp`. This will automatically split the text into a generator of tokens (one token to each word). These tokens will have the part of speech already tagged in it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With over 2.7 million residents, it is also the most populous city in both the state of Illinois and the Midwestern United States\n",
      "With ADP\n",
      "over ADP\n",
      "2.7 NUM\n",
      "million NUM\n",
      "residents NOUN\n",
      ", PUNCT\n",
      "it PRON\n",
      "is VERB\n",
      "also ADV\n",
      "the DET\n",
      "most ADV\n",
      "populous ADJ\n",
      "city NOUN\n",
      "in ADP\n",
      "both ADJ\n",
      "the DET\n",
      "state NOUN\n",
      "of ADP\n",
      "Illinois PROPN\n",
      "and CCONJ\n",
      "the DET\n",
      "Midwestern PROPN\n",
      "United PROPN\n",
      "States PROPN\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(chicago_sents[1])\n",
    "print(doc)\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to convert this into a set of part of speech tags, we could add in a little extra Python to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ADP': 4, 'NUM': 2, 'NOUN': 3, 'PUNCT': 1, 'PRON': 1, 'VERB': 1, 'ADV': 2, 'DET': 3, 'ADJ': 2, 'PROPN': 4, 'CCONJ': 1}\n"
     ]
    }
   ],
   "source": [
    "tags = {}\n",
    "for token in doc:\n",
    "    if token.pos_ not in tags.keys():\n",
    "        tags[token.pos_] = 1\n",
    "    else:\n",
    "        tags[token.pos_] += 1\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many more tags to that `spaCy` can provide for us:\n",
    "\n",
    "- Text: The original word text.\n",
    "- Lemma: The base form of the word.\n",
    "- POS: The simple part-of-speech tag.\n",
    "- Tag: The detailed part-of-speech tag.\n",
    "- Dep: Syntactic dependency, i.e. the relation between tokens.\n",
    "- Shape: The word shape – capitalisation, punctuation, digits.\n",
    "- is alpha: Is the token an alpha character?\n",
    "- is stop: Is the token part of a stop list, i.e. the most common words of the language?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text\tLemma\tPOS\tDetailed POS\tDependency\tShape\tIs alphabetic?\tIs stopword?\n",
      "With\twith\tADP\tIN\tprep\tXxxx\tTrue\tTrue\n",
      "over\tover\tADP\tIN\tquantmod\txxxx\tTrue\tTrue\n",
      "2.7\t2.7\tNUM\tCD\tcompound\td.d\tFalse\tFalse\n",
      "million\tmillion\tNUM\tCD\tnummod\txxxx\tTrue\tFalse\n",
      "residents\tresident\tNOUN\tNNS\tpobj\txxxx\tTrue\tFalse\n",
      ",\t,\tPUNCT\t,\tpunct\t,\tFalse\tFalse\n",
      "it\t-PRON-\tPRON\tPRP\tnsubj\txx\tTrue\tTrue\n",
      "is\tbe\tVERB\tVBZ\tROOT\txx\tTrue\tTrue\n",
      "also\talso\tADV\tRB\tadvmod\txxxx\tTrue\tTrue\n",
      "the\tthe\tDET\tDT\tdet\txxx\tTrue\tTrue\n",
      "most\tmost\tADV\tRBS\tadvmod\txxxx\tTrue\tTrue\n",
      "populous\tpopulous\tADJ\tJJ\tamod\txxxx\tTrue\tFalse\n",
      "city\tcity\tNOUN\tNN\tattr\txxxx\tTrue\tFalse\n",
      "in\tin\tADP\tIN\tprep\txx\tTrue\tTrue\n",
      "both\tboth\tADJ\tPDT\tpobj\txxxx\tTrue\tTrue\n",
      "the\tthe\tDET\tDT\tdet\txxx\tTrue\tTrue\n",
      "state\tstate\tNOUN\tNN\tappos\txxxx\tTrue\tFalse\n",
      "of\tof\tADP\tIN\tprep\txx\tTrue\tTrue\n",
      "Illinois\tillinois\tPROPN\tNNP\tpobj\tXxxxx\tTrue\tFalse\n",
      "and\tand\tCCONJ\tCC\tcc\txxx\tTrue\tTrue\n",
      "the\tthe\tDET\tDT\tdet\txxx\tTrue\tTrue\n",
      "Midwestern\tmidwestern\tPROPN\tNNP\tcompound\tXxxxx\tTrue\tFalse\n",
      "United\tunited\tPROPN\tNNP\tcompound\tXxxxx\tTrue\tFalse\n",
      "States\tstates\tPROPN\tNNP\tconj\tXxxxx\tTrue\tFalse\n"
     ]
    }
   ],
   "source": [
    "print('\\t'.join(['Text', 'Lemma', 'POS', 'Detailed POS', 'Dependency',\n",
    "                'Shape', 'Is alphabetic?', 'Is stopword?']))\n",
    "for token in doc:\n",
    "    print('\\t'.join([token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "          token.shape_, str(token.is_alpha), str(token.is_stop)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check For Understanding 1 (10 minutes)\n",
    "\n",
    "With a partner, do the following:\n",
    "\n",
    "1. Pick two different wikipedia articles\n",
    "2. Get the content using the `wikipedia` library\n",
    "3. Using `spacy`, derive the following:\n",
    "    1. How many tokens are in your article?\n",
    "    2. How many parts of speech are in each article? How often do they occur?\n",
    "    3. As a percentage of the total number of tokens, how often does each part of speech occur?\n",
    "4. Does it look like there's a difference across your documents? What other types of documents would have different distributions of parts of speech?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Donald John Trump (born June 14, 1946) is the 45th and current President of the United States, in office since January 20, 2017. Before entering politics, he was a businessman and television personality.\n",
      "Trump was born in the New York City borough of Queens. He earned an economics degree from the Wharton School of the University of Pennsylvania. A third-generation businessman, Trump followed in the footsteps of his grandmother Elizabeth and father Fred in running the family real estate company. \n"
     ]
    }
   ],
   "source": [
    "trump = wikipedia.page('Donald Trump')\n",
    "print(trump.content[0:500])\n",
    "trump_model = nlp(trump.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Barack Hussein Obama II (US:  ( listen) bə-RAHK hoo-SAYN oh-BAH-mə; born August 4, 1961) is an American politician who served as the 44th President of the United States from 2009 to 2017. He is the first African American to have served as president. He previously served in the U.S. Senate representing Illinois from 2005 to 2008 and in the Illinois State Senate from 1997 to 2004.\n",
      "Obama was born in 1961 in Honolulu, Hawaii, two years after the territory was admitted to the Union as the 50th state.\n"
     ]
    }
   ],
   "source": [
    "obama = wikipedia.page('Barack Obama')\n",
    "print(obama.content[0:500])\n",
    "obama_model = nlp(obama.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trump_tags = {\n",
    "    'all': 0\n",
    "}\n",
    "\n",
    "obama_tags = {\n",
    "    'all': 0\n",
    "}\n",
    "\n",
    "for token in trump_model:\n",
    "    trump_tags['all'] += 1\n",
    "    if token.pos_ not in trump_tags.keys():\n",
    "        trump_tags[token.pos_] = 1\n",
    "    else:\n",
    "        trump_tags[token.pos_] += 1\n",
    "        \n",
    "for token in obama_model:\n",
    "    obama_tags['all'] += 1\n",
    "    if token.pos_ not in obama_tags.keys():\n",
    "        obama_tags[token.pos_] = 1\n",
    "    else:\n",
    "        obama_tags[token.pos_] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tags, Trump: 16542 Obama: 15898\n"
     ]
    }
   ],
   "source": [
    "print('Total tags, Trump:', trump_tags['all'], \n",
    "     'Obama:', obama_tags['all'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trump tags\n",
      "all 16542 100.00\n",
      "PROPN 2409 14.56\n",
      "PUNCT 2023 12.23\n",
      "VERB 2038 12.32\n",
      "NUM 636 3.84\n",
      "DET 1196 7.23\n",
      "NOUN 2705 16.35\n",
      "CCONJ 468 2.83\n",
      "ADJ 1267 7.66\n",
      "ADP 2008 12.14\n",
      "PRON 318 1.92\n",
      "SPACE 269 1.63\n",
      "ADV 424 2.56\n",
      "PART 306 1.85\n",
      "SYM 468 2.83\n",
      "INTJ 1 0.01\n",
      "X 6 0.04\n"
     ]
    }
   ],
   "source": [
    "print('Trump tags')\n",
    "for k, v in trump_tags.items():\n",
    "    print(k, v, '%.2f' % (v * 100 / trump_tags['all']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obama tags\n",
      "all 15898 100.00\n",
      "PROPN 2699 16.98\n",
      "PUNCT 1829 11.50\n",
      "SPACE 263 1.65\n",
      "VERB 1663 10.46\n",
      "ADP 2159 13.58\n",
      "NOUN 2563 16.12\n",
      "INTJ 3 0.02\n",
      "NUM 766 4.82\n",
      "DET 1182 7.43\n",
      "ADJ 1122 7.06\n",
      "PRON 227 1.43\n",
      "PART 294 1.85\n",
      "ADV 321 2.02\n",
      "CCONJ 432 2.72\n",
      "SYM 368 2.31\n",
      "X 7 0.04\n"
     ]
    }
   ],
   "source": [
    "print('Obama tags')\n",
    "for k, v in obama_tags.items():\n",
    "    print(k, v, '%.2f' % (v * 100 / obama_tags['all']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Looks like more Proper Nouns for Obama and more verbs for Trump. Very interesting!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entities\n",
    "\n",
    "Named entities are business, people, countries, or other things that refer to a specific person, place, or thing (think `Apple`, computer manufacturer versus `apple`, delicious crunchy fruit). `spaCy` can identify named entities for us, which we can either highlight or drop from our analyses. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've parsed a string of text using `spaCy`, we can call out the named entities using the `.ents` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With over 2.7 million residents, it is also the most populous city in both the state of Illinois and the Midwestern United States <class 'spacy.tokens.doc.Doc'>\n"
     ]
    }
   ],
   "source": [
    "print(doc, type(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over 2.7 million residents MONEY\n",
      "Illinois GPE\n",
      "the Midwestern United States ORG\n"
     ]
    }
   ],
   "source": [
    "for named_entity in doc.ents:\n",
    "    print(named_entity.text, named_entity.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`spaCy` provides a set of labels for each type of named entity:\n",
    "\n",
    "|Label|Description|\n",
    "|:-- | :-- |\n",
    "|PERSON |\tPeople, including fictional. |\n",
    "|NORP |\tNationalities or religious or political groups. |\n",
    "|FACILITY |\tBuildings, airports, highways, bridges, etc. |\n",
    "|ORG |\tCompanies, agencies, institutions, etc. |\n",
    "|GPE |\tCountries, cities, states. |\n",
    "|LOC |\tNon-GPE locations, mountain ranges, bodies of water. |\n",
    "|PRODUCT |\tObjects, vehicles, foods, etc. (Not services.) |\n",
    "|EVENT |\tNamed hurricanes, battles, wars, sports events, etc. |\n",
    "|WORK_OF_ART |\tTitles of books, songs, etc. |\n",
    "|LAW |\tNamed documents made into laws. |\n",
    "|LANGUAGE |\tAny named language.|\n",
    "|DATE |\tAbsolute or relative dates or periods. |\n",
    "|TIME |\tTimes smaller than a day. |\n",
    "|PERCENT |\tPercentage, including \"%\".\n",
    "|MONEY |\tMonetary values, including unit. |\n",
    "|QUANTITY |\tMeasurements, as of weight or distance. |\n",
    "|ORDINAL |\t\"first\", \"second\", etc. |\n",
    "|CARDINAL |\tNumerals that do not fall under another type. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to see all the unique named entities in the Chicago page, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chicago ( ( listen) or ), officially the City of Chicago, is the third-most populous city in the United States. With over 2.7 million residents, it is also the most populous city in both the state of Illinois and the Midwestern United States. It is the county seat of Cook County. The Chicago metropolitan area, often referred to as Chicagoland, has nearly 10 million people and is the third-largest in the U.S. Chicago has often been called a global architecture capital. Chicago is considered one o'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chicago.content[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'2011', 'Kenwood', 'the National Register of Historic Places', 'Poetry', 'Inland Northern American English', 'City in a Garden', 'Calumet Harbor', 'Missouri Valley Conference', 'Daily News', 'Julius Rosenwald', 'Harry Caray', '1,045,560', 'half', 'The Chicago', '=\\n\\n71% of', 'Swedes', '315,000 square feet', '1.2% to 431 (', 'Tower, Museum of Science and Industry', \"O'Hare Airport\", '1966', 'Democratic Party', 'Newark', 'Chicago International Airport', 'Transport Efficiency', 'National Louis University', 'December 2016', 'American Literature', 'West Ridge', 'Harold Washington College', 'U.S.', 'June 2017', '29.7% of', 'Dietetics, American Association of Nurse Anesthetists', 'Hindus', 'calendar year 2014', 'Robert Morris University Illinois', 'John Farwell', '176.2', '7% identity', 'the \"Founder of Chicago\"', 'the Great Lakes region', '$2.5 billion', '2005', '160', 'Willis', '1997', 'Jackson Parks', '93', \"O'Hare\", 'Devon Avenue', '\\nChicago Public Schools', 'Amrany and Rotblatt-Amrany', 'Institute of Puerto Rican Arts', '44,103', 'ComEd', 'Natural', 'Checker', 'John Hancock Center', 'the Chicago Park District', '1992', 'Illinois Institute of Technology', 'Jane Byrne', 'African-Americans', 'five years', '1953-54', 'Little Calumet River', '5% decrease', '1919', '24.3', 'November', 'American College of Surgeons', '16th', 'Millennium Park', 'the first half of 2013', '672', 'January', 'WSCR', 'Hyde Park', 'fewer than 200', 'Bennett', 'Lithuanian', 'the Chicago Sanitary', 'the United Center', '25 miles', 'South Shore Line', 'the Chicago Building', 'the Chicago River', 'Archer Daniels Midland', 'the Feltre School', 'Potawatomi', '4,000', 'ESPN Radio-', '1908', 'Armour', 'UIC', 'One', 'Brookfield', 'I.O.', 'Today', 'Monument', 'Orlando', '1933', '\\n45.0%', '2,695,598', '1927', '8', 'SouthtownStar', 'Landfill', 'the Sinaloa Cartel', 'the Chicago History Museum', 'Anton Cermak', 'CBS', 'Miami', 'South Side', 'Five', 'Taylor Street', 'the year', '1955', '65', 'Jean Baptiste Point', 'Roger Brown', '6th', '2014', '2009', '1876', '6-mile (', 'Queen of Peace High School', \"the U.S. Census Bureau's\", '2008', 'one day', 'Cook', 'New Orleans', 'Montenegrins', 'across 26 miles', 'Fox', 'The Chicago Blackhawks of', '1.1% Indian', 'the Treaty of Greenville', 'Meštrović', 'Iroquois Landing Lakefront Terminal', 'the Eastern United States', '2016', 'Batcolumn', 'Yellow Cab Companies', '250 million US gallons', 'Urbs', 'Metaxa', \"the United States'\", '1926', 'Rockford', '52', 'Jesse Brown', '1989', 'Logan Square Boulevards Historic District', 'Hispanic', 'Groupon', '1975', 'the 1940s', '1871', 'Central Park', 'Bridgeview', 'Grant Park Music Festival', 'the Performing Arts Oriental Theatre', 'Central Chicago', 'Lithuanian Chicagoans', \"Ann & Robert H. Lurie Children's\", 'West Loop', 'Illinois State', 'Cook County Jail', 'Chicago Rockford International Airport', 'the Shedd Aquarium', '28.9%.', 'summer', '900 linear meters (', 'the Northern District', 'Six', 'Indian', 'over 200', 'Jim Nutt', 'Midway Airport', 'African Americans', 'Copernicus', '435', '1837', 'Poles', 'the 19th century', 'Navy', 'Garfield Park Conservatory', 'the Jefferson Township', 'World Trade Center', '44th', '1939', 'the Adler School of Professional Psychology', 'South', 'The Midwestern University Chicago College of Osteopathic Medicine', 'Galena and Chicago Union Railroad', 'CREATE', \"Frank Gehry's\", 'several consecutive days', 'Dow Jones Indexes', 'Water Tower Place', 'Feedburner', 'National Register of Historic Places', 'the Tribune Broadcasting-', 'the Brookfield Zoo', '77', 'the Chicago Region Environmental', '1979', 'Richard Teller Crane', 'annual African American', 'The South Side', 'the Great Depression', 'Claes Oldenburg', '355', 'Paseo Boricua', 'Plan', 'Electronic Dance Music', 'Thorvaldsen', 'The first', '1974', 'The Metra Electric Line', '1860', 'Abraham Lincoln', 'the Museum of Contemporary Art', 'Christopher Columbus', 'Midwestern', 'June', 'three days', '1906', 'the Encyclopedia of Chicago', 'UBS', 'Desi', \"Wacław Szymanowski's\", 'Ojibwe', '0.2% Honduran', 'Blackhawks', 'Peoples Gas', 'African-American', 'ER', 'Latino', 'seven years later', '2013', 'Bears', 'WMVP', '79', 'the Schwinn Bicycle Company', 'Barack Obama', 'Academy of General Dentistry', 'Navy Pier', '2014–2016', 'Northwestern Memorial Hospital', '98', '\\n5.5% Asian', 'Carmel High School', '0.2% Salvadoran', 'Chase Bank', 'World', '1848', '\\n0.5% American', '\\n13.4% from', '\\n\\nChicago Wilderness', 'sixteen', 'Art Institute of Chicago', 'the Chicago Stock Exchange', 'Crate & Barrel', 'between 1958 and early 1962)', 'the Chicago Architecture Foundation', 'Porter', 'Prohibition', 'Imagist', 'Loop', 'the end of the 19th century', \"Dallin's Signal of Peace\", '3.6 million', 'Utilities', 'the Lyric Opera of Chicago', 'Republicans', 'South Side Chicago', '2006', 'CareerBuilder', 'the Chicago Tribune', 'Time', 'American College of Healthcare Executives', 'General Electric', '410', '2010–11', \"Marshall Field's\", \"Benjamin Ferguson's\", 'Andersonville', '2016 World', 'daily', '1866', 'the Rehabilitation Institute of Chicago', '9th', '15.65', 'nearly 25,000', 'Jean Dubuffet', 'COMEX', 'Chicago High School', '233,903', 'Cosmos', 'Blue', 'Midway', '24‑hour', 'the Chicago Sister', 'sixth', 'the 1870s and 1880s', 'Commonwealth Edison', 'the University of Illinois', 'The Onion', 'third', '1874', '390', 'Staff', '\\n28.9% Hispanic', 'DW60 Ferris', 'The Cook County', '510', 'The Bob Newhart Show', 'WGN America', 'Whitney M. Young', 'William Butler Yeats', \"Saint-Gaudens's\", 'John Marshall', 'the University of Chicago Divinity School', '5', 'Hong Kong', 'Dan Ryan', 'Detroit', 'the Chicago State Cougars', 'Midwest', 'Maxwell Street Polish', 'The Chicago-style', 'Tom Memorial Park', 'The American Medical Association', 'NBA', '579 ft (', 'Yellow', '47,074', '80', '\"The Loop\"', 'the Chicago Mercantile Exchange', 'McKenna', 'Cadillac Palace Theatre', 'the Evangelical Covenant Church', 'ImprovOlympic', '4,331', '1930', 'Watch Dogs and', 'Lollapalooza', '1688', 'Indianapolis', 'the Dziennik Związkowy', 'the Fine Arts Building', 'Florida', 'Washington', '13.2', 'thousands', 'John Ashbery', \"Claire's\", 'Heald Square Monument', 'Ivan Albright', 'less than 25% of', 'The Head of State', 'Eugene Sawyer', 'NowSecure', 'the 1880s and 1890s', 'Democratic', 'Milwaukee', '1869', '1795', 'The River North Gallery District', 'The Chicago Bulls of', 'Ellen Gates Starr', 'today', 'the Pitchfork Music Festival', 'Miegs Field', 'Cleveland', 'Kane', '21.4% Mexican', 'Orbitz', 'Chicagoans', 'the Chicago Board Options Exchange', 'Indiana', 'Arts', 'DuPage', '== Education', 'The Chicago Marathon', 'the 1920s and 1930s', 'Black Belt', '26th Street', 'Chase Tower', 'Lake Calumet Harbor', 'Czechs', '780,000 square meters', '62.8% increase', 'Ten years later', 'Victory Gardens Theater', 'Crunelle', 'the 1840s', 'weeks', 'Green, Orange', 'The City of Chicago', 'the Commodities Exchange Inc.', 'Stritch School of Medicine', 'Las Vegas', 'Montgomery Ward', 'Chicago Fire', 'Bank of America Theatre', '1983', 'Major League Soccer', '1950', 'Irv Kupcinet', 'the Adler Planetarium & Astronomy Museum', '943', 'Republican National Convention', '1850 to 1890', 'Harvey College', 'Lions', 'Rice', 'Seattle', 'Egyptian', 'Resurrection High School', 'the American Civil War', 'Irish', 'Objectivist', 'Chicago Festival Ballet', 'about one', 'Columbian Exposition', 'the mid-nineteenth century', 'about 200', 'Near North Side', 'the Green Bay Packers', 'the Chicago Bears', '1889', 'the Goodman Theatre', '12.5% increase', 'Michigan Avenue', 'Brioschi', '416', '6 miles', '97', 'the American South', 'GE Healthcare', 'the spring', \"St. Adalbert's\", '4.0', 'the Seventh District', 'Bronzeville', 'yearly', 'St. Ignatius College Preparatory School', 'the Chicago Metropolitan Area', 'RTA', 'Academy of Nutrition', '40 km', 'Flying Dragon', 'HQ', 'Beyond the Beltway with', 'Chicago Cultural Center', 'Loyola University Chicago', 'roughly 60% of', '1942', 'Marshall Field', 'the National Mall', 'the Flag of Chicago', 'Boystown', \"New York's\", 'first', '1812', 'Bulls', 'Ida Crown', 'Morgan Park Academy', 'Ottawa', 'The Roman Catholic Archdiocese of Chicago', 'the Federal Reserve', 'Major League', 'Blue Island', 'Grand Calumet River', 'July', 'Willis Tower', '400,545', 'Chicago Public Radio', '21st', 'Common Council', 'Cubs', '54,188', '34', \"Ferris Bueller's\", '10,000 or more', \"Frédéric Chopin's\", 'The A.V. Club', 'the British School of Chicago', 'CBOT', 'Reagan', '\\n2.7% from', 'the Home Insurance Building', 'West Sides', 'two', '75', 'South Bend', 'Showtime', 'the Chicago Portage', 'Streeterville', '1929', 'Chodzinski', 'the end of 2018', '500', 'four', '10-acre', 'the Chicago Black Renaissance', 'Illinoisan', 'Thanksgiving', '176.5', 'The Chicago Bears', 'Dutch Wheels', 'Art Nouveau', '1,000,000', 'Chicagoland', 'Millions', \"St. Valentine's Day Massacre\", 'Chinatown', 'the American League', 'the Chicago Public Schools', 'Daley Plaza', 'Kraft Heinz', 'New York', 'the South Side', '(Eisenhower', 'Loop district', 'Jens Ludwig', 'Caribbean', 'more than 77% were', 'Shimer College', '16', 'University of Illinois', 'the Federal ATF', 'World Marathon Majors', '32', 'Amrany', 'the Standard Oil Building', 'the Chicago Picasso', '1945', 'HALS', 'the Hyde Park Township', 'Big East Conference', 'United Airlines', 'City Colleges of Chicago', 'Super Bowl XX', 'Kearney', '1905', 'sixty years', '294', 'the Illinois River', 'the beginning of', '0.3% Cuban', 'Seoul', 'Americans', 'the Chicago Sun-Times', 'the City Beautiful Movement', 'the Polish Museum of America', '35', '1.6% Chinese', 'Health & Society', 'About 18.3%', 'May 4, 1886', 'Bugs Moran', 'the Southern United States', 'Charlie Trotter', '66', 'Lincoln Park Conservatory', 'African', 'Park Zoo', 'Lake Shore Drive', 'Lincoln Park', 'Deerfield', 'MasterCard Worldwide Centers of Commerce Index', '1687', 'Time Out Chicago', 'Chicagoua', 'seven years', 'Kendall', 'December 20, 2014', '15.94', 'The Blues Brothers', 'the Democratic Party', '205', 'June 2016', 'Mean Girls, Wanted, Batman Begins, The Dark Knight, Transformers: Dark of the Moon, Transformers: Age of Extinction, Transformers: The Last Knight, Divergent, Batman v Superman: Dawn of Justice, Sinister 2 and Suicide Squad', '88', 'Dfa', 'The McLaughlin Group', 'Wisconsin', '−27 °F', 'the Steppenwolf Theatre Company', 'The Chicago Fire Soccer Club', '10,000', 'Allium', '1965', '$1.95 billion', 'Vietnam', 'the Sears Tower', 'the Great Lakes', 'the American Geographical Society Library\\nHistoric American Landscapes Survey', 'the Chicago Police Department', 'the DuSable Museum of African American History', 'nearly 400 acres', '1892', 'Michigan Canal', '45', 'the Midway Plaisance', '50.17 million', 'Bobby Hull', 'Obama', 'League', 'Superfans', 'Dow 30 company', 'the Museum Campus', 'nearly 150 percent', '1980', 'The Chicago Tribune', 'Lawrence Avenue', 'The North Side', 'Southern,', 'The West Side', 'Singapore', 'Broadway', 'De La Salle Institute', 'Upscale', 'Fairbanks', 'Burnham', 'Christians', 'the 20th century', 'State Street', '38.9', 'the Feinberg School of Medicine', 'CTA', 'The Museum Campus', 'North Avenue', 'the Evangelical Lutheran Church in America', 'the last half of the 19th century', 'DuSable Park', 'February 1856', 'Wabash Avenue Bridge', '97 km', 'the 1910s', 'Gateway Theatre', 'Democrats', '1,200 acres', 'Department of Transportation', '20th and 21st centuries', 'Sauk', '55', 'NHL', 'Cristo Rey Jesuit', '5b', '41', 'the Chicago Cubs', 'the Bulls (91', '910', '0.1% Thai)', 'the Ford Center', '90s', '75.8', 'Merc', '0.4% Korean', 'Museum Campus', 'FIFA', 'Chicago', 'Greektown', 'Wrigley Field', 'Two years later', 'Sears Tower', 'Union Station', 'Abakanowicz', 'Maywood', 'the Field Museum of Natural History', 'Richard M. Daley', '0.3% Pakistani', 'Jaume Plensa', 'the Obama Foundation', 'Near West Side', 'the Saturday Night', 'Illinois State Board of', 'Charles B. Atwood', '0.2% Japanese', 'NPR', 'March 1937', '2004', \"Anish Kapoor's\", 'North Park University', 'Moore', '1 million', '14 million bushels', 'The University of Illinois College of Medicine', 'Chicago Academy', 'The Chicago Sky', '43', '2005–2009', 'Symphony Center', 'more than 570', 'Rick Tramonto', 'Northwestern University', 'Strachovský', 'between 2010 and 2040', 'Notable', 'annually', 'Horto', '2012', '0.2% Peruvian', 'Saint-Gaudens', 'LED', '1995', 'Little Italy', 'Agora', 'Richard J. Daley', 'Peoria', 'night', 'New York Electric Air Line', 'the Museum of Broadcast Communications', 'Ford Motor Company', 'about 4.48 million workers', 'the Chicago Reader', 'U.S. Receiver of Public Monies', 'Magdalena Abakanowicz', 'Lane Technical College Prep High School', '\\n\\nThe Port of', \"O'Hare Airports\", 'May 16, 2011', \"Dion O'Banion\", 'the Barack Obama Presidential Center', '600', 'Grant Achatz', '1900', 'The Illinois Medical District', 'the Lycée Français de Chicago', 'World Series', 'Walk Score', 'WGN', 'Ogden Avenue', 'Northside College Preparatory High School', 'Western Athletic Conference', 'Brown, Purple', 'ten', '1 mile', 'approximately 153,000', 'Jack Brickhouse', 'Northeastern Illinois University', '94', 'three', 'the Kinzie Street Bridge', '2019', '200', \"Lamb Chop's\", 'Washington Park', '2006 WNBA season', \"North America's\", 'Cook County Circuit Court', 'Michelin Guide 3 Star Award', 'Miró', 'Best Sports City', 'more than 100,000', 'the early 1960s', 'GE Transportation', 'the Buehler Center', '55 percent', 'Robert Lostutter', 'San Antonio', 'Grundy', 'the National Hockey League', '2,900', 'the Midwestern United States', 'Trump International Hotel', 'Discovery Channel', 'Martin Luther King', 'Jane Addams', 'North', 'Ireland', 'Stan Mikita', 'Tiffany', '1970', 'LGBT', 'Two', '20', 'North Side', 'American', 'NYMEX', 'the early 20th century', 'World Cities Research Network', 'Oak Park', 'Cloud Gate', 'the Chicago Regional Port District', 'D.C.', 'The Chicago Lincoln', 'the Driehaus Museum', 'more than 14% of', 'Malcolm X College', 'the Kansas–Nebraska Act', 'Manhattan', 'the winter season', 'AT&T Plaza', '105', 'four consecutive years form 2013 to 2016', 'the late 1920s', 'US', '1.72 million', 'Rush University Medical Center', \"The Alarm, Polasek's memorial to\", 'Summers', 'the Loyola Ramblers', 'North Chicago', 'The Oprah Winfrey Show', '3,000 linear feet', 'Francis W. Parker School', '22nd', 'The Near West Side', 'Europe', '1679', 'Warsaw', 'Latin', 'John Crerar', 'about 300', 'between 1910 and 1920', 'Century of Progress International Exposition Worlds Fair', 'Eternal Silence', 'WFLD', 'ULTA Beauty', 'Condé Nast Traveler', 'Abbott Laboratories', 'Brewster', 'Lincoln', 'the University of Chicago Medical Center', 'the Pritzker Military Library', '32.9% in', 'William Rainey Harper', 'New York City', 'Jefferson Park', 'the Harris Theater for Music and Dance', 'Ace Hardware', 'Lithuanians', 'between 1920 and 1930', 'Northerly Island', 'the Mississippi River', '2010', '17 Financial', 'Greek', 'Mike', 'U.S. Department of Transportation', 'Stanley', 'Between 1910 and 1930', 'the Northwestern Wildcats', 'recent years', '1987', '2020', 'Pilsen', 'four distinct seasons', 'Cascia High School', 'La Villita', 'Carbondale', 'Rate Field', 'LaPorte', 'tunneling two miles', 'Fort Dearborn', 'Baxter International', 'Democrat', 'Lorado Taft', 'NBC', 'The Willis Tower', 'Belmont Avenue', 'Albany Park', 'Jews', '452', 'Armour Square', 'Wilbur Wright College', 'The Loop', 'Miro', '290', 'Republican', 'Italians', '1956', 'the Chicago Shakespeare Theater', '42', 'Enrico Fermi', 'Langston Hughes', 'Ed Paschke', 'DePaul College Prep', 'about $658.6 billion', 'September 10, 2012', 'Red', 'Frank Lloyd Wright', 'the United States', '18th', 'seventh', 'June 15, 1835', 'Moose', 'North Side Chicago', 'Shikaakwa', 'the Southwest Side', '1993', 'fourth', 'Ogden', '2008–2012', 'Sneak Previews', 'Chicago Board of Health', 'NFL', 'Northern American', 'between 1955 and 1971', 'over 3.6 million', '20 million', 'the twentieth century', 'Machine', \"T. S. Eliot's\", '24 hours', 'Metra', 'Polish', 'Prison Break', '42,063', 'March 4, 1837', 'one', 'Sears', 'fifth', '92', 'Bowman', 'West Town', '1873', 'DePaul Blue Demons', 'Four', 'Grace', 'North America', \"Northern Indiana Commuter Transportation District's\", 'Home Alone', 'John Whitfield Bunn', '1924', 'First', 'the mid-18th century', 'the McCormick Place Convention Center', 'PBS', 'several square miles', '750', \"Richard J. Daley's\", '2.5', 'Grant Park', 'every year', 'The Fourth Presbyterian Church', 'Calumet', 'America', 'Marist High School', 'Integrys Energy Group', 'CME Group', 'number one', 'the Chicago Imagists', 'The University of Chicago Oriental Institute', 'the Chicago Freedom Movement', 'Each year', 'every day of the year', '1940 to 1979', '1867', 'Elston', '3,000', 'the Allstate Arena', 'Leon Golub', 'Site Selection', 'the Cook County Forest', '2001', 'Iroquois County', 'McHenry', 'Pink', 'the Great Lakes Megalopolis', 'Douglas', 'StreetWise', '1995 to 2008', '29,000 square meters', '2', 'Harpo Studios', '100 acres', 'twenty-four', 'South Shore', '762', 'the Catholic Theological Union', 'Chicago State University', 'The 2015 year-end', 'WBBM', '0.40', 'the Lutheran School of Theology', 'the Chicago Literary Renaissance', 'British', 'Haymarket', '\\nList of fiction', 'the National Football League', 'Horizon League', 'the Chicago Cardinals', 'The Chicago Police Department', 'Sufjan Stevens', 'the National League', 'Grand Rapids', 'World War I', 'Robert de LaSalle', '490', 'Western Avenue', 'PRI', 'USDA', '10% down', '96', 'Pizzeria Uno', 'Stephen Douglas', 'River North', 'about 70', '2016 World Series', 'the Chicago Opera Theater and', 'The Chicago Loop', 'the Institute of Gerontology of Ukraine', 'Divvy', 'the 1850s and 1860s', 'the Chicago Climate Exchange', 'the Cumulus Media-owned', 'Foreign trade zone', 'the Miami-Illinois', 'the White', '2007', 'the Art Institute of Chicago', 'Northwest Side', 'Peggy Notebaert', '1885', 'the University of Chicago Crime', 'the Chicago Board of Trade Building', 'Rohe', '54 million', 'WYCC', 'Havliček', 'Bruce DuMont', 'the National Weather Service', 'CBS Radio-', 'Fountain', 'about 4 miles', 'Crusader', 'early 1920s', 'Helmut Jahn', 'the West Coast', '47,408', 'Broadway In', 'Harold Washington', 'North American', 'nearly two-thirds', 'about 130', '\"National Universities\"', 'the Chicago White Sox', 'Accreditation Council for Continuing Medical Education', 'Checagou', 'The Good Wife', 'Draugas', 'Serbs', 'Chicago Cityscape, Chicago, Cook County', 'John Root', '61.7', '55.7', 'Downers Grove', '1883', '22.1% of', 'Rahm Emanuel', '50', 'the Illinois International Port District', 'Blue Cross', '7', 'the City of Chicago', 'Ravinia Festival', '26%)', 'Urbana', 'Lake Michigan', 'September 9, 2013', 'White House', 'The University of Chicago', 'the Windy City Times', 'Kennedy–King College', 'Cook County', 'LocalWiki Local Chicago Wiki', '1897', 'the Chicago School', 'the 1990s', '1,000,000 m3', '27.5 million', 'about 300,000', '125', '31.7% in', 'Spearman', \"O'Hare International Airport\", '16.14', 'Sunday', 'seven', 'Much', 'Harlem', 'Rush University', 'Exelon', '0.3% Vietnamese', 'the Chicago Defender', 'the Chicago City Council', '31.7% non', 'Hull House', '458', 'August 12, 1833', '28', 'Labor', 'Soldier Field', 'Early Edition', \"Edward Kemys's\", 'extends 60 miles', '415', 'South Halsted Street', '1998', 'Crown Fountain', 'MLS', '2013–2014 20th', '10.4', 'hundreds', '$13', '397', '1934', '1983–1987', '3', 'the Soviet Union', 'Alinea', '\\n\\nSporting News named Chicago', 'the century', 'McCormick Place', 'the Cleveland Indians', '1920s', 'the School of the Art Institute of Chicago', 'Law', 'Walter Payton', 'eight', 'about 2 days', 'CBOE', 'ThyssenKrupp North America', '2002', 'Chagall', 'Cabrini', 'December 2, 1942', '0.3% Colombian', 'the Windy City', 'roughly 4% of', '2010–11 season', '1.1% Filipino,', '11-line', 'Walgreens', '1977', 'Adlai Stevenson', 'M.D.. Rauch', 'Albanians', '9.7 km', '11', 'Flamingo', '55th', 'Cella', 'Derrick Rose', 'Solidarity Promenade', \"the Illinois State's\", 'the year 2016', 'the U.S. Futures Exchange', 'Al Capone', 'West Side Chicago', 'Gwendolyn Brooks', 'Jones College Prep', 'The Lithuanian Opera Company', 'Amtrak', 'U.S. News & World Report', 'more than 6,000', 'The Merchandise Mart', 'Oldenburg', 'the Calumet River', '22% have', 'nineteen boulevards', '16 percent', 'the University of Chicago', 'Du Sable', 'The Institute for Clinical Social Work', 'Buddhists', 'MacCormac College', '0.7% Guatemalan,', 'Sable', 'Ezra Pound', 'the summer of 2016', 'the Brass Era', 'The White', 'Roosevelt University', 'the Great Chicago Fire of 1871', '1917', 'Croatians', 'The CME Group', 'Mother Teresa', 'Caterpillar Inc.', '578', 'the last', \"Humboldt Park's\", 'James Merrill', 'French', 'American Society for Clinical Pathology', 'American Dental Association', 'the United States—', 'Anish Kapoor', 'four years', 'Jay Pritzker Pavilion', 'East', 'Parks', '109', 'The Joffrey Ballet', 'Truman College', 'twenty-six', 'Lake', 'DePaul University', 'Joseph Jefferson Awards', 'Daniel Burnham', 'Logan', 'Germans', 'the University of Chicago Laboratory Schools', '1968', 'Pontiac', '11.09 million', 'eleven', 'eight seasons', 'The Chicagoland Chamber of Commerce', 'the Chicago Teachers Union', 'since 1992', 'William Carlos Williams', 'the University of Chicago Cultural Policy Center', 'the Chicago Race Riot', '19', 'nine', 'Toyota Park', 'Chopin Park', 'Michael Jordan', 'the Illinois Institute of Art – Chicago', '90% black', 'Illinois', 'the National Basketball Association', '57', 'Midway International Airport', 'Democratic National Convention', '200th', 'the Erikson Institute', 'DW60', 'American Community Survey', 'July 24, 1934', '60', '1910', 'Greyhound Lines', 'The Chicago Symphony Orchestra', 'Chicago Hope', '1:1‑scale', '32.6% of', '4', 'Royal Baths', 'Perfect Strangers and its spinoff Family Matters, Punky Brewster, Married... with Children, Kenan', 'Calder', \"Oprah Winfrey's\", 'Kennedy', 'the American Hospital Association', 'Puerto Rican-', 'the Moody Bible Institute', 'Standing', 'U.S. House', '468', '18.5', 'the Chicago Public Library', 'more than 4,000', 'World War II', 'the University of Illinois Medical Center', 'Rochester', 'the Black Belt', 'National Basketball Association', 'Pace', 'The Second City', 'as many as 21', 'the Chicago Board of', '46', 'Köppen', '860-880', 'the Willis Tower', 'Global Cities Index', '22', '2003', 'Choose Chicago', 'The Robie House', 'Bugmobile', '201 meters', 'February 23, 2011', '1893', 'Tony Accardo', 'the Federal Reserve Bank of Chicago', 'over 8,000 acres', '970', 'Saint Xavier University', '\\n\\nThe Regional Transportation Authority', 'the Kansas City Southern Railway', '16.5', '3 million', 'July 2016', 'Frédéric Chopin', 'Muslims', 'Saturday', 'Taste of Chicago festival', 'Division I', '38-foot (', 'Advanced Placement', 'all three', 'each year', 'the Chicago Loop', 'over 2.7 million residents', 'Rogers Park', '1912', 'Henri Joutel', 'the National Museum of Mexican Art', '31', 'Port Huron', 'Franklin D. Roosevelt', 'Accreditation Council for Graduate Medical Education', 'Preston Bradley Hall', 'the U.S. Army Corps of', '1.308 million', 'June 4, 1998', 'Bosnians', 'United Continental Holdings', 'Mexican', 'The Chicago School of Professional Psychology', 'Rosemont', '29% since', 'extends 10 feet', 'Baroque', '1872', 'five 50,000', 'United States', \"Loyola University Chicago's\", 'the Northwest', '== Infrastructure', 'up to 506', '19th century', 'Street on', 'Hegewisch', 'The City Council', 'McDonald', 'The Chicago Public Library', 'the last two decades', 'three World Series', 'Mother McAuley Liberal Arts High School', 'Newcity', 'the Tribune Media', 'approximately 196', 'nearly 10 million', 'Catholic', 'John H. Rauch', '1984', 'the Aon Center', 'early 2018', 'about 75% of', 'the Chicago Fire Department', 'up to eight adults', 'Lake Calumet', '449', '100,000', 'Chicago Dance Crash', 'Harriet Monroe', 'January 20, 1985', 'Molly', 'Richard J. Daley College', 'the Arizona Cardinals', 'the Chicago Botanic Garden', 'winter', 'Jesuit', 'Renowned Chicago', 'Wood Street', '1969', 'The Frugal Gourmet', 'the Chicago Pride Parade', 'Plensa', 'American Osteopathic Association', 'about $640 billion', 'Mies', 'Rick Bayless', 'More than half', '190', 'Columbia College Chicago', 'the 1780s', 'Uptown', '2.7 million', '53', 'Seven', 'Puerto Rican', 'Alexander Calder', 'The Illinois International Port', 'the Harris Theater', 'two-thirds', 'the Northwest Indian War', 'GRAB', 'the North Side', 'Kościuszko', 'Major League Baseball', 'This Great Migration', 'Portland', 'about 3,500', 'Robb Report', 'Josephinum Academy', 'fifty', 'American League', 'Bill Savage', 'five', 'the Museum of Science and Industry', '2,600', 'The Civic Opera House', 'the Polish Cathedral', 'Quincy', 'the Constitution', 'The Chicago Board of', 'AP', 'Boyle', 'Auditorium Building', 'six', 'Clark Street', '90', 'Healthcare', 'more than one', 'Combo', '1803', '0.6% Ecuadorian', 'St. Louis', 'The Illinois Department of Tourism', '1907', '1985', 'Beast', 'Lutheran', 'Chicagou', 'the Great Northern Migration (Saar)', '1833', 'Barbara Rossi', 'The City Beautiful', 'Dalai Lama', 'the Prairie School', 'the Society for Human Rights', 'the forty years', 'ABC', 'several decades', \"Puerto Rican People's Parade\", 'Michelle Obama', 'Tribune', 'Fortune Global 500', \"Dubuffet's\", 'the Hubbard Street Dance Chicago', '\"The Love Song of J. Alfred Prufrock\"', 'only one', 'Visitor Information Center', '12', \"Bill Swerski's\", 'the Great Chicago Fire', 'Gary', '448', '2015', 'the 1850s', 'WGN-TV', '1994', 'the Latin School of Chicago', 'the Daily Herald', '30,000', 'Wrigley', '59% in', '58', 'John H. Stroger', '3.8% Puerto', 'Presbyterian', 'Kankakee', '\\n32.9% Black', '(800,000 barrels', 'Albert Raby', 'about 29', 'Blue Shield Association', 'the Chicago School of', 'the 1980s', \"Prentice Women's\", 'the Chicago Medical School', 'The Chicago Transit Authority', '580', 'Glencoe', '1816', 'Buckingham Fountain', 'San Francisco', '98.1% of', 'annual', 'Boeing', 'the United States of', 'Edmund Dick Taylor', 'WLS', 'National Blue Ribbon School', 'the Humboldt Park', 'West University', 'Eastern Europe', 'the Chicago', 'Native American', 'John Paul II', 'the New York Mercantile Exchange', 'second', 'Vesuvio', 'CSO', '29.3% of', 'the Port District', 'AM', '1901', 'Marc Chagall', '\\n\\n\\n', '42 km', 'Iowa', 'Burnham Park', 'Megabus', 'the U.S. Department of Education', 'Louis Sullivan', 'Los Angeles', 'the Western Hemisphere', 'Sixteen Candles', '8,390,000 square feet', 'early summer', 'Jackson Park', 'Boston', 'between 1851 and 1920', 'Nuclear Energy', '5,800', 'Tower', 'Italian', 'Carl Sandburg', 'more than US$13.7 billion', 'Joliet Junior College', 'William Thompson', \"Los Angeles as America's\", 'the summer'} 1528\n"
     ]
    }
   ],
   "source": [
    "chicago_model = nlp(chicago.content)\n",
    "named_entities = []\n",
    "for entity in chicago_model.ents:\n",
    "    named_entities.append(entity.text)\n",
    "print(set(named_entities), len(set(named_entities)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for Understanding 2 (10 Minutes)\n",
    "\n",
    "As a market (or as groups in your market), please discuss the following:\n",
    "\n",
    "1. As modelers, we will frequently have to make decisions about how to transform data. If you were using NLP to predict things, would it make sense to keep named entities? Would it make sense to drop them? If it would depend on the circumstances, under what circumstances would it make sense to keep or drop named entities?\n",
    "\n",
    "We'll have a couple of markets come on mic to discuss cases they identified where keeping named entities might make sense and cases where it would not make sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `textblob` to do sentiment analysis\n",
    "\n",
    "We can also use a library known as [`textblob`](https://textblob.readthedocs.io/en/dev/) to do a **lot** of text transformation and extraction on our behalf. For our purposes, we are going to use it to analyze text and derive the overall sentiment of the text.\n",
    "\n",
    "Sentiment can be split into two related scales:\n",
    "\n",
    "- subjectivity (0 to 1): scores closer to 0 are more objective in tone, scores closer to 1 are more subjective in tone\n",
    "- polarity (-1 to 1): scores closer to -1 are more negative in tone, closer to 0 are more neutral, and closer to 1 are more positive in tone.\n",
    "\n",
    "Using `textblob` is user-friendly -- pass a string into a `Textblob()` class and then call the `.sentiment.polarity` or `sentiment.subjectivity` attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "really_good_review = '''\n",
    "Goodness me, what a fantastic movie. \n",
    "Caught the world premiere at the Toronto International Film Festival and \n",
    "the entire theater laughed until they cried. \n",
    "Amazingly directed, HILARIOUSLY funny, it blends a 1930s gangster \n",
    "stylishness into a Hong Kong kung fu movie to astonishing results. \n",
    "Who would've thought you could top Shaolin Soccer? \n",
    "Not me, until I saw this movie. Stephen Chow pulled it off. \n",
    "Chow's comedic timing gets better and better with every movie \n",
    "he makes, and while his films are depending more and more on \n",
    "CGI these days, and makes this movie much more a fantasy kung \n",
    "fu film than a traditional one, it hardly detracts from the \n",
    "enjoyable experience. Make it your mission to see this film - \n",
    "it will be one of the most entertaining you ever see. \n",
    "I can't remember the last film I enjoyed myself in more. \n",
    "My eyes still hurt from wiping away tears of laughter. Seriously.  \n",
    "'''\n",
    "\n",
    "really_bad_review = '''\n",
    "Thank you for coming into your performance review Mr. Smith.\n",
    "The company is concerned about your performance. Lately your work has \n",
    "been subpar and at times counter to this company's stated goals.\n",
    "Your demeanor has been aggresive and at times hostile to your \n",
    "fellow coworkers.\n",
    "We have no choice but to terminate your employment, effective \n",
    "immediately. Thank you.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5749999999999998 0.33295454545454545\n",
      "0.7 0.15\n"
     ]
    }
   ],
   "source": [
    "good_review = TextBlob(really_good_review)\n",
    "print(good_review.sentiment.subjectivity, good_review.sentiment.polarity)\n",
    "\n",
    "bad_review = TextBlob(really_bad_review)\n",
    "print(bad_review.sentiment.subjectivity, bad_review.sentiment.polarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for Understanding 3 (5 Minutes)\n",
    "\n",
    "Individually, please answer the following:\n",
    "\n",
    "1. What type of subjectivity and polarity scores would you expect wikipedia articles to have?\n",
    "2. Confirm your hypothesis by using `textblob` on some of the wikipedia pages we have used so far. Were your thoughts confirmed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3412846858940613 0.10109983766233746\n"
     ]
    }
   ],
   "source": [
    "chicago_textblob = TextBlob(chicago.content)\n",
    "print(chicago_textblob.sentiment.subjectivity, chicago_textblob.sentiment.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3528265439047681 0.08126754382568327\n"
     ]
    }
   ],
   "source": [
    "trump_textblob = TextBlob(trump.content)\n",
    "print(trump_textblob.sentiment.subjectivity, trump_textblob.sentiment.polarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Instructor answer:\n",
    "\n",
    "I expected relatively neutral tones and subjectivity, and even for Trump, those expectations held out\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding these features to DataFrames\n",
    "\n",
    "We may want to include these features into a DataFrame for use in a later model. The most straightforward way to do so would be to apply them using Pandas.\n",
    "\n",
    "Here, we'll make use of the same dataset on economic news that we used yesterday. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relevance</th>\n",
       "      <th>headline</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Yields on CDs Fell in the Latest Week</td>\n",
       "      <td>NEW YORK -- Yields on most certificates of dep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>The Morning Brief: White House Seeks to Limit ...</td>\n",
       "      <td>The Wall Street Journal OnlineThe Morning Brie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Banking Bill Negotiators Set Compromise --- Pl...</td>\n",
       "      <td>WASHINGTON -- In an effort to achieve banking ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Manager's Journal: Sniffing Out Drug Abusers I...</td>\n",
       "      <td>The statistics on the enormous costs of employ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Currency Trading: Dollar Remains in Tight Rang...</td>\n",
       "      <td>NEW YORK -- Indecision marked the dollar's ton...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   relevance                                           headline  \\\n",
       "0          1              Yields on CDs Fell in the Latest Week   \n",
       "1          0  The Morning Brief: White House Seeks to Limit ...   \n",
       "2          0  Banking Bill Negotiators Set Compromise --- Pl...   \n",
       "3          0  Manager's Journal: Sniffing Out Drug Abusers I...   \n",
       "4          1  Currency Trading: Dollar Remains in Tight Rang...   \n",
       "\n",
       "                                                text  \n",
       "0  NEW YORK -- Yields on most certificates of dep...  \n",
       "1  The Wall Street Journal OnlineThe Morning Brie...  \n",
       "2  WASHINGTON -- In an effort to achieve banking ...  \n",
       "3  The statistics on the enormous costs of employ...  \n",
       "4  NEW YORK -- Indecision marked the dollar's ton...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "econ = pd.read_csv('datasets/economic_news.csv',\n",
    "                  usecols=[7, 11, 14],\n",
    "                  nrows=200)\n",
    "econ['text'] = econ['text'].apply(lambda x: x.replace('</br>', ''))\n",
    "econ['relevance'] = econ['relevance'].apply(lambda x: 1 if x == 'yes' else 0)\n",
    "econ.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(econ[['text']],\n",
    "                                                   econ['relevance'],\n",
    "                                                   test_size=0.50,\n",
    "                                                   random_state=8675309)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use `spaCy` to create a column for the number of monetary-based named entities, followed by using `textblob` to create a polarity score for each article.\n",
    "\n",
    "While we can try to put this into a lambda function, it will probably be easiest in this case to define four functions and apply them.\n",
    "\n",
    "However, because we're sequentially loading up each row of data and processing it, this can be a little bit of a time and memory sink. Expect processing to take some extra time for this step.*\n",
    "\n",
    "* **note**: for spacy, there are faster ways to process the data that do not involve pushing it through Pandas. Investigate the spacy `pipe` method if you're looking to do a larger amount of text transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def number_of_monetary_ents(text):\n",
    "    text = nlp(text)\n",
    "    return len([x.text for x in text.ents if x.label_ == 'MONEY'])\n",
    "\n",
    "def polarity(text):\n",
    "    text = TextBlob(text)\n",
    "    return text.sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train['num_monetary'] = X_train['text'].apply(number_of_monetary_ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    100.00000\n",
       "mean       1.44000\n",
       "std        2.16641\n",
       "min        0.00000\n",
       "25%        0.00000\n",
       "50%        0.00000\n",
       "75%        2.00000\n",
       "max        9.00000\n",
       "Name: num_monetary, dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train['num_monetary'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1301c2cf8>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD3CAYAAADxJYRbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRQx9SaqIoS9JFTH0Jaki/w/ruVc1EcigTQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1301ca780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train['num_monetary'].plot(kind='hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train['polarity'] = X_train['text'].apply(polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1300b9ef0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD3CAYAAADxJYRbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1300c0438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train['polarity'].describe()\n",
    "X_train['polarity'].plot(kind='hist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also pass this into a predictive model to see if these features can assist predicting economic status:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.95\n",
      "[[79  0]\n",
      " [ 5 16]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      1.00      0.97        79\n",
      "          1       1.00      0.76      0.86        21\n",
      "\n",
      "avg / total       0.95      0.95      0.95       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(X_train[['num_monetary', 'polarity']], y_train)\n",
    "print(rfc.score(X_train[['num_monetary', 'polarity']], y_train))\n",
    "predictions = rfc.predict(X_train[['num_monetary', 'polarity']])\n",
    "print(confusion_matrix(y_train, predictions))\n",
    "print(classification_report(y_train, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6\n",
      "[[57 10]\n",
      " [30  3]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.66      0.85      0.74        67\n",
      "          1       0.23      0.09      0.13        33\n",
      "\n",
      "avg / total       0.52      0.60      0.54       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_test['num_monetary'] = X_test['text'].apply(number_of_monetary_ents)\n",
    "X_test['polarity'] = X_test['text'].apply(polarity)\n",
    "print(rfc.score(X_test[['num_monetary', 'polarity']], y_test))\n",
    "predictions = rfc.predict(X_test[['num_monetary', 'polarity']])\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: you're probably wondering _why_ we would try these things when they don't seem to immediately help. During a larger project, we will likely spend days if not weeks on feature extraction and analysis and will want to make as many useful features as possible to make as good a model as possible. Other techniques may involve more nuanced modeling, such as looking at the sequence of parts of speech, etc. Part of this lesson is designed to expose to what is out there so that when faced with a situation where those techniques may be useful, you're aware of their existence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assigning documents to topics using LDA\n",
    "\n",
    "LDA (Latent Dirichlet Allocation) is an unstructured machine learning technique that iteratively attempts to find clusters of words that are likely to happen together across multiple documents. We interpret the co-occurance of these words together to be analgous to different topics discussed in across a body of documents. \n",
    "\n",
    "LDA works by iteratively guessing how likely a given word is to be part of a given topic until we tell it to stop. \n",
    "\n",
    "This process of updating probabilities will make more sense after next weeks lectures on Bayes, but we'll quickly discuss here and move forward.\n",
    "\n",
    "(Explanation cribbed from [Introduction to Latent Dirichlet Allocation](http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/))\n",
    "\n",
    "We begin by picking a set of documents and a number of topics that we want to generate. One way that we do this is what's known as collapsed Gibbs saampling. We do the following:\n",
    "\n",
    "1. Randomly assign every word in every document to one of the $k$ topics:\n",
    "    - $w$: a word in a document\n",
    "    - $d$: a document\n",
    "    - $k$: a topic\n",
    "2. At this point, every word has a likelihood that they belong in a given a topic, based on the other words in documents that they exist in. \n",
    "3. Iterate through every word in every document and:\n",
    "    1. Assume that every other word has the correct likelihood that they belong to each topic (so, `apple` might have a distribution of `[0.1, 0.1, 0.2, 0.4, 0.2]` for five topics.\n",
    "    2. Look at the likelihood of seeing word $w$ in document $d$ and adjust the topic probabilities as needed\n",
    "    > for example, if there are a lot of words in topic 1 in document $d$ and word $w$ has a stronger likelihood of being in topic 2, because we're assuming that every **other** distribution is correct, we should change our understanding of where word $w$ belongs and tweak it more in favor of belonging to topic 1, not topic 2\n",
    "    \n",
    "You can kind of interpret this with an analogy:\n",
    "\n",
    "> Imagine you move to a new town and you don't know what sort of people you want to hang out with. You imagine there's five different groups of people. You start visiting different places around town (the park, the library, the mall, etc.) and noting who's there. Everytime you go to a place you start adjusting your expectation on who you'll see there (such as the goths constantly are at the mall, so we should expect less and less that they'll show up at the library). This is (very roughly) analgous to what LDA is doing.\n",
    "\n",
    "The name latent dirichlet allocation should begin to make more sense in this context:\n",
    "- latent -- because we have no explicit marker of topic and are grouping things together based on features we are inferring, not seeing\n",
    "- [dirichlet](https://en.wikipedia.org/wiki/Dirichlet_distribution) -- is a type of probability distribution for multiple vectors at once (like a bunch of words towards a bunch of topics)\n",
    "- allocation -- we are allocating different words to different topics via this iterative updating of priors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both sklearn and `gensim`, a library we will discuss in the context of a technique called `word2vec`, can handle LDA. However, we'll rely on the sklearn implementation here to reduce the amount of extra work we'll need to do in picking up a new library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's reimport all of the economic news data instead of just the first 200 rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NEW YORK -- Yields on most certificates of dep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Wall Street Journal OnlineThe Morning Brie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WASHINGTON -- In an effort to achieve banking ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The statistics on the enormous costs of employ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NEW YORK -- Indecision marked the dollar's ton...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  NEW YORK -- Yields on most certificates of dep...\n",
       "1  The Wall Street Journal OnlineThe Morning Brie...\n",
       "2  WASHINGTON -- In an effort to achieve banking ...\n",
       "3  The statistics on the enormous costs of employ...\n",
       "4  NEW YORK -- Indecision marked the dollar's ton..."
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "econ = pd.read_csv('datasets/economic_news.csv',\n",
    "                  usecols=[14])\n",
    "econ['text'] = econ['text'].apply(lambda x: x.replace('</br>', ''))\n",
    "econ.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll transform the data using `CountVectorizer` and removing stop words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<8000x46379 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 802395 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer(stop_words='english')\n",
    "cv.fit(econ['text'].values)\n",
    "X = cv.transform(econ['text'].values)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll instantiate an LDA and fit it to our sparse matrix of words. We have to provide a number of topics that we are looking for (in this case, we're looking for 5 topics). We'll also store the names of the each of the words created during the `CountVectorizer` step for use with the LDA results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/richardharris/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7, learning_method=None,\n",
       "             learning_offset=10.0, max_doc_update_iter=100, max_iter=10,\n",
       "             mean_change_tol=0.001, n_components=5, n_jobs=1,\n",
       "             n_topics=None, perp_tol=0.1, random_state=None,\n",
       "             topic_word_prior=None, total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = cv.get_feature_names()\n",
    "lda = LatentDirichletAllocation(n_components=5)\n",
    "\n",
    "lda.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of our work will be held in the `.components_` feature. Each row of this array is one of our topics and each column (in order) is a word created by `CountVectorizer`. The values are the relative \"likelihoods\" that the word $w$ should be in topic $t$.\n",
    "\n",
    "> From the sklearn docs, `.components_`: \"can be viewed as pseudocount that represents the number of times word j was assigned to topic i. It can also be viewed as distribution over the words for each topic after normalization\" (we could normalize by dividing row total for that topic). \n",
    "\n",
    "For our purposes, it's enough to say that bigger values means the word belongs more in that topic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 46379)\n"
     ]
    }
   ],
   "source": [
    "print(lda.components_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = pd.DataFrame(lda.components_,\n",
    "                      columns=feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see what words are most likely in each topic, we could sort by the biggest values for each topic.\n",
    "\n",
    "> Every feature has a likelihood of being in a topic, just a very, very low one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0\n",
      "said tax president budget house federal billion year government new administration deficit congress spending bush state plan mr million ыєs years pay yesterday public taxes \n",
      "\n",
      "Topic 1\n",
      "percent unemployment years year people jobs workers economy labor 000 economic time work recession says job new business inflation washington rate just said like companies \n",
      "\n",
      "Topic 2\n",
      "fed bank said rates federal dollar reserve economic banks economy inflation markets policy financial rate money trade chairman central world growth new market term treasury \n",
      "\n",
      "Topic 3\n",
      "market stock year said new percent prices stocks average york million investors rate index rates rose quarter week billion dow points trading fell month shares \n",
      "\n",
      "Topic 4\n",
      "ыќ ыєs ыє ыєt ыў company ыу рк ыпthe gas ыпwe ыуthe ыч ыпi ыєre stores 10 ыпit television tv network ing corp steel oils \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for topic in range(5):\n",
    "    print('Topic', topic)\n",
    "    word_list = results.T[topic].sort_values(ascending=False).index\n",
    "    print(' '.join(word_list[0:25]), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we change the number of topics, we should see the topics change slightly. Remember that because this is an unstructured technique our editorial power as the modeler is important to identify useful topics. \n",
    "\n",
    "However, this provides a powerful tool to create summaries of larger bodies of documents!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for Understanding 4 (20 Minutes)\n",
    "\n",
    "In pairs, do the following:\n",
    "\n",
    "1. Rerun the LDA, choosing 10 topics instead of 5. \n",
    "> Make sure that you can explain what each line of the code does to each other. This can be as generic as \"This runs an LDA with 10 components on a matrix of words and documents\" but it's important to be able to explain what a block of code is doing. In particular, make sure that you're able to explain what has happened in this line of code above `word_list = results.T[topic].sort_values(ascending=False).index` -- if you need to, start with the very first portion (`results`) and investigate what each subsequent step does.\n",
    "2. Look at the results of your LDA. How would you summarize what each topic says?\n",
    "3. Does 10 look to be a correct number of topics? Are the same words showing up in multiple topics? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/richardharris/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0\n",
      "dollar trade japan world currency foreign china yen japanese europe european countries markets global international euro central nations german bank germany west asia currencies asian \n",
      "\n",
      "Topic 1\n",
      "company said stock million companies market securities new corp financial bank billion mortgage investors street firm buy wall firms group mr york exchange based public \n",
      "\n",
      "Topic 2\n",
      "steels rails aircrafts lo vs ыў tion ment vietnam eu coppers depressing arid libor ethanol si pi ыу pf geneva ruble att sexual biker tlie \n",
      "\n",
      "Topic 3\n",
      "county race drug cio afl col oils day immigrants cola men state chart woman children police glendening immigration arc ыєve town man davis teams ehrlich \n",
      "\n",
      "Topic 4\n",
      "market said year rates rate prices percent new economy inflation investors economic fed federal growth week stock yesterday reserve month billion york higher quarter bond \n",
      "\n",
      "Topic 5\n",
      "president said federal tax budget house administration congress government deficit bush chairman mr committee yesterday economic senate billion policy board white reserve officials political clinton \n",
      "\n",
      "Topic 6\n",
      "year years percent people time new 000 said economic unemployment workers says economy jobs work business income just recession like washington money american states government \n",
      "\n",
      "Topic 7\n",
      "authors buchanan еи foreclosures patman tended wallich lifetime retiring miners journalists eisenhower debit inequality er pretax payday churchill oh mastercard hate skidded speakers trash millennium \n",
      "\n",
      "Topic 8\n",
      "ыєs ыќ ыє ыєt ыу ыў ыуthe ыпthe рк ыпwe ыуa rails ыєre ыпit ыќthe today ыпa employes material ыуin ыуwhich ыуbut ыпin ыпthere du \n",
      "\n",
      "Topic 9\n",
      "stock million stocks shares dow average market points new index york jones nasdaq cents industrial today trading issues earnings 500 rose share day exchange close \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CfU 1\n",
    "\n",
    "# X has not changed so we'll jump to the LDA fitting process\n",
    "# Explanation of each line in comments\n",
    "\n",
    "#instantiates an LDA with 10 components\n",
    "lda = LatentDirichletAllocation(n_components=10) \n",
    "\n",
    "# fits the LDA to our matrix of word counts\n",
    "lda.fit(X) \n",
    "\n",
    "# creates a dataframe of our results. \n",
    "# The columns are each word from CountVectorizer\n",
    "# The rows are the values for each topic\n",
    "# The values in the cells is the relative likelihood that it belongs there\n",
    "results = pd.DataFrame(lda.components_,\n",
    "                      columns=feature_names) \n",
    "\n",
    "# iterate over each topic\n",
    "for topic in range(10):\n",
    "    print('Topic', topic)\n",
    "    # transpose results (so that the topics are the columns)\n",
    "    # isolate the column for the specific topic we want\n",
    "    # sort from biggest to smallest\n",
    "    # get the indices (which are the words)\n",
    "    # the values for the cells are not particularly useful\n",
    "    word_list = results.T[topic].sort_values(ascending=False).index\n",
    "    # join the first 25 words together as they are the strongest topic words\n",
    "    print(' '.join(word_list[0:25]), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Instructor answer for 2 and 3\n",
    "\n",
    "Topic interpretations:\n",
    "\n",
    "Topic 0 -- looks like economic reports \n",
    "Topic 1 -- central bank discussions\n",
    "Topic 2 -- global trade, specifically asia and europe\n",
    "Topic 3 -- elections and political parties\n",
    "Topic 4 -- tech companies and stocks (maybe IPOs?)\n",
    "Topic 5 -- garbled words\n",
    "Topic 6 -- international news, sports, Israeal\n",
    "Topic 7 -- actual stock market movements\n",
    "Topic 8 -- a mix of automotive industry and international aid\n",
    "Topic 9 -- government budgets and deficits\n",
    "\n",
    "10 topics look pretty cohesive and easy to understand. We may want one or two more as some of them are a little mixed, but this works out decently well. We also apparently have some encoding issues that would be good to iron out.\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py36]",
   "language": "python",
   "name": "Python [py36]"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
