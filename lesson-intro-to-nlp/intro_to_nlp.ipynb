{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, all imports for this notebook are below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, \\\n",
    "HashingVectorizer, TfidfVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import make_union, make_pipeline\n",
    "\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Natural Language Processing \n",
    "\n",
    "Natural language processing is a fairly large field and one that we will not cover in nearly enough detail this week. For our purposes, it involves the processing of written language into numbers that we will then use during modeling. This puts it squarely in the preprocessing / feature engineering stage.\n",
    "\n",
    "> Note, there are plenty of cases where we may want to predict what the next word of a statement will be or to automatically provide an answer to a question, program a chat bot, etc. These also fall under the NLP umbrella, but for this week's discussions, we'll focus on just going from language -> numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simple Example\n",
    "\n",
    "Suppose we are building a spam/ham classifier. Input are emails, output is a binary classification of whether or not the email is a spam message or not.\n",
    "\n",
    "Here's an example of an input email:\n",
    "\n",
    "> Hello, I saw your contact information on LinkedIn. I have carefully read through your profile and you seem to have an outstanding personality. This is one major reason why I am in contact with you. My name is Mr. Valery Grayfer Chairman of the Board of Directors of PJSC \"LUKOIL\". I am 86 years old and I was diagnosed with cancer 2 years ago. I will be going in for an operation later this week. I decided to WILL/Donate the sum of 8,750,000.00 Euros(Eight Million Seven Hundred And Fifty Thousand Euros Only etc. etc.\n",
    "\n",
    "and a second example:\n",
    "\n",
    "> Hello, I am writing in regards to your application to the position of Data Scientist at Hooli X. We are pleased to inform you that you passed the first round of interviews and we would like to invite you for an on-site interview with our Senior Data Scientist Mr. John Smith. You will find attached to this message further information on date, time and location of the interview. Please let me know if I can be of any further assistance. Best Regards.\n",
    "\n",
    "We might want to look at certain words and determine where they happen:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for Understanding 1 (10 Minutes)\n",
    "\n",
    "With a partner, do the following:\n",
    "\n",
    "1. Identify which email is a spam email and which is a ham email. \n",
    "2. What words exist in the spam email that do not exist in the ham email? What about the reverse?\n",
    "3. Using these words, create a rule that would classify a spam email and do not share it with your partner.\n",
    "> For example, the rule could be \"If the email refers to cancer, it is a spam email\"\n",
    "4. Go online (or into your inbox!) and find a second example of a spam email. PM this text to your partner. Does their rule correctly classify this email?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bag-of-words model is a simplifying representation used in natural language processing. In this model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity. In other words, how many times does each word happen?\n",
    "\n",
    "One way to do this easily is with the `Counter` module in the standard library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'i': 7, 'of': 4, 'and': 3, 'your': 2, 'contact': 2, 'have': 2, 'to': 2, 'an': 2, 'this': 2, 'is': 2, 'am': 2, 'in': 2, 'with': 2, 'the': 2, 'years': 2, 'etc.': 2, 'hello,': 1, 'saw': 1, 'information': 1, 'on': 1, 'linkedin.': 1, 'carefully': 1, 'read': 1, 'through': 1, 'profile': 1, 'you': 1, 'seem': 1, 'outstanding': 1, 'personality.': 1, 'one': 1, 'major': 1, 'reason': 1, 'why': 1, 'you.': 1, 'my': 1, 'name': 1, 'mr.': 1, 'valery': 1, 'grayfer': 1, 'chairman': 1, 'board': 1, 'directors': 1, 'pjsc': 1, '\"lukoil\".': 1, '86': 1, 'old': 1, 'was': 1, 'diagnosed': 1, 'cancer': 1, '2': 1, 'ago.': 1, 'will': 1, 'be': 1, 'going': 1, 'for': 1, 'operation': 1, 'later': 1, 'week.': 1, 'decided': 1, 'will/donate': 1, 'sum': 1, '8,750,000.00': 1, 'euros(eight': 1, 'million': 1, 'seven': 1, 'hundred': 1, 'fifty': 1, 'thousand': 1, 'euros': 1, 'only': 1})\n",
      "\n",
      "\n",
      "Counter({'to': 5, 'of': 4, 'you': 4, 'the': 3, 'i': 2, 'data': 2, 'scientist': 2, 'we': 2, 'and': 2, 'further': 2, 'hello,': 1, 'am': 1, 'writing': 1, 'in': 1, 'regards': 1, 'your': 1, 'application': 1, 'position': 1, 'at': 1, 'hooli': 1, 'x.': 1, 'are': 1, 'pleased': 1, 'inform': 1, 'that': 1, 'passed': 1, 'first': 1, 'round': 1, 'interviews': 1, 'would': 1, 'like': 1, 'invite': 1, 'for': 1, 'an': 1, 'on-site': 1, 'interview': 1, 'with': 1, 'our': 1, 'senior': 1, 'mr.': 1, 'john': 1, 'smith.': 1, 'will': 1, 'find': 1, 'attached': 1, 'this': 1, 'message': 1, 'information': 1, 'on': 1, 'date,': 1, 'time': 1, 'location': 1, 'interview.': 1, 'please': 1, 'let': 1, 'me': 1, 'know': 1, 'if': 1, 'can': 1, 'be': 1, 'any': 1, 'assistance.': 1, 'best': 1, 'regards.': 1})\n"
     ]
    }
   ],
   "source": [
    "spam = '''\n",
    "Hello, I saw your contact information on LinkedIn. I have carefully read through your profile and you seem to have an outstanding personality. This is one major reason why I am in contact with you. My name is Mr. Valery Grayfer Chairman of the Board of Directors of PJSC \"LUKOIL\". I am 86 years old and I was diagnosed with cancer 2 years ago. I will be going in for an operation later this week. I decided to WILL/Donate the sum of 8,750,000.00 Euros(Eight Million Seven Hundred And Fifty Thousand Euros Only etc. etc.\n",
    "'''\n",
    "\n",
    "ham = '''\n",
    "Hello, I am writing in regards to your application to the position of Data Scientist at Hooli X. We are pleased to inform you that you passed the first round of interviews and we would like to invite you for an on-site interview with our Senior Data Scientist Mr. John Smith. You will find attached to this message further information on date, time and location of the interview. Please let me know if I can be of any further assistance. Best Regards.\n",
    "'''\n",
    "\n",
    "print(Counter(spam.lower().split()))\n",
    "print('\\n')\n",
    "print(Counter(ham.lower().split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, outside of prepositions and articles, most words are unique across these documents. However, for the ham email, the following interesting words happen more than once:\n",
    "\n",
    "- data\n",
    "- scientist\n",
    "- further\n",
    "\n",
    "and in the spam email:\n",
    "\n",
    "- contact\n",
    "- years\n",
    "- etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-learn Options\n",
    "\n",
    "Scikit-learn offers two different options for Bag-of-Words approaches to NLP:\n",
    "\n",
    "- [`CountVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\n",
    "- [`HashingVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html)\n",
    "\n",
    "In **most** cases, you'll be using the `CountVectorizer` object, but there may be times, especially if you are on an older machine, that `HashingVectorizer` will be a huge timesaver.\n",
    "\n",
    "> **Major Note**: NLP can take a lot of computing power. Don't be afraid to look for alternatives like `HashingVectorizer`, to cut your dataset down, to use fewer words, use a larger virtualized machine (such as on AWS). Working smart, not hard applies very well here!\n",
    "\n",
    "[`CountVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) will take a set of words and split them up into one column per word, with (by default) the count of the word for that row in that column. We'll walk through a brief example now and then dive into the options. \n",
    "\n",
    "This library is very full-featured and has a number of options to set or tweak -- get used to reviewing those options to make sure that you are gaining the most out of your work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nHello, I saw your contact information on Lin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\nHello, I am writing in regards to your appli...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  \\nHello, I saw your contact information on Lin...\n",
       "1  \\nHello, I am writing in regards to your appli..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame().from_dict({0: spam, 1: ham}, \n",
    "                             orient='index')\n",
    "df.columns = ['text']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x111 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 129 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer()\n",
    "cv.fit(df['text'])\n",
    "cv.transform(df['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is sparse matrix? Transformations like this tend to create matrices with a very large amount of zeroes (words may only show up once or twice across a number of documents, so a lot of extra memory is expended to represent a very large number of zeroes).\n",
    "\n",
    "Sparse matrices collapse regular (dense) matrices by marking down only cases where a non-zero value is found for a certain combination of row and column. It then drops all the zeroes, allowing for a reduced memory footprint.\n",
    "\n",
    "We can call `.todense()` on the output to convert it into a dense matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 1, 1, 1, 1, 2, 2, 3, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 2,\n",
       "         0, 0, 1, 1, 1, 1, 1, 2, 2, 1, 0, 0, 1, 0, 1, 1, 2, 1, 0, 1, 0, 2,\n",
       "         0, 1, 0, 0, 0, 2, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 4,\n",
       "         1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "         1, 0, 0, 1, 0, 2, 2, 1, 1, 0, 2, 1, 1, 0, 1, 1, 2, 2, 0, 0, 2, 2, 2],\n",
       "        [0, 0, 0, 0, 0, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,\n",
       "         2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 2, 0, 0, 0, 1, 1, 0, 1, 1,\n",
       "         1, 1, 2, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 4,\n",
       "         0, 2, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 2, 1, 0, 2, 0, 1,\n",
       "         0, 1, 1, 0, 1, 3, 1, 0, 0, 1, 5, 0, 0, 2, 0, 0, 1, 1, 1, 1, 0, 4, 1]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.transform(df['text']).todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we can grab the feature names and apply them if we are interested in seeing the distribution of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>750</th>\n",
       "      <th>86</th>\n",
       "      <th>ago</th>\n",
       "      <th>am</th>\n",
       "      <th>an</th>\n",
       "      <th>and</th>\n",
       "      <th>any</th>\n",
       "      <th>application</th>\n",
       "      <th>...</th>\n",
       "      <th>we</th>\n",
       "      <th>week</th>\n",
       "      <th>why</th>\n",
       "      <th>will</th>\n",
       "      <th>with</th>\n",
       "      <th>would</th>\n",
       "      <th>writing</th>\n",
       "      <th>years</th>\n",
       "      <th>you</th>\n",
       "      <th>your</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 111 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   00  000  750  86  ago  am  an  and  any  application  ...   we  week  why  \\\n",
       "0   1    1    1   1    1   2   2    3    0            0  ...    0     1    1   \n",
       "1   0    0    0   0    0   1   1    2    1            1  ...    2     0    0   \n",
       "\n",
       "   will  with  would  writing  years  you  your  \n",
       "0     2     2      0        0      2    2     2  \n",
       "1     1     1      1        1      0    4     1  \n",
       "\n",
       "[2 rows x 111 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(cv.transform(df['text']).todense(), \n",
    "             columns=cv.get_feature_names()) # use .get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why the random numbers at the front? That comes from the `8,750,000.00` and `86` sections of the spam email -- our parser does not interpret `8,750,000.00` as one number but rather the string of words `[8, 750, 000, 00]`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `CountVectorizer` Options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CountVectorizer` takes the following (useful) keyword arguments:\n",
    "\n",
    "| Argument | Default Value | Definition |\n",
    "| :--- | :--- | :--- |\n",
    "| `decode_error` | `strict` | What to do if text cannot be decoded. `strict` will raise a `UnicodeDecodeError`, `ignore` will skip that word, `replace` will attempt to replace it with a non-Unicode variant|\n",
    "| `strip_accents` | `None` | When preprocessing a word, `CountVectorizer` does nothing with the accented characters. `ascii` will convert those characters if they have a direct ASCII mapping (à -> a, for example), and `unicode` is slower but will do it for all characters | \n",
    "| `preprocessor / tokenizer` | `None` | Ways to override how to split text into words (`tokenizer`) and what to do with those words before vectorizing (`preprocessor`) -- we'll discuss shortly |\n",
    "| `ngram_range` | `(1, 1)` | Sometimes we may want each sequence of _n_ words as well as each individual word. This is known as an _ngram_ and we cna set that here | \n",
    "| `stop_words` | `None` | Whether or not to remove stop words. Will discuss later. |\n",
    "| `max_df` | `1.0` | `df` refers to the document frequency -- how often does a given word show up across documents. If we set this to a float less than 1.0, any word that happens more frequently than that value will be discarded. Any integer will be the number of documents instead of the proportion | \n",
    "| `min_df` | `1` | Same as `max_df`, but for the number / proportion of documents that a word has to appear in before it is included |\n",
    "| `max_features` | `None` | The top _n_ occuring features to include. If `None`, include all features. This is a great way to coerce `CountVectorizer` to return a matrix of a specific shape / size |\n",
    "| `binary` | `False` | If `True`, return dummy variables instead of a count of occurances |\n",
    "\n",
    "Whew! That's a lot!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for Understanding 2 (15 Minutes)\n",
    "\n",
    "For this Check for Understanding (and many others), we'll be using the [`20 Newsgroups`](http://scikit-learn.org/stable/datasets/twenty_newsgroups.html) dataset. This dataset includes a number of old messages from a set of newsgroups and is a standard dataset to practice NLP techniques with. \n",
    "\n",
    "For this first section, we will be using the first 100 messages from the `sci.space` newsgroup. We'll also be stripping them of any headers, footers, or quoted messages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading dataset from http://people.csail.mit.edu/jrennie/20Newsgroups/20news-bydate.tar.gz (14 MB)\n"
     ]
    }
   ],
   "source": [
    "space = fetch_20newsgroups(subset='train',\n",
    "                          categories=['sci.space'],\n",
    "                          remove=('headers', 'footers', 'quotes'))\n",
    "space_messages = space['data'][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Any lunar satellite needs fuel to do regular orbit corrections, and when\n",
      "its fuel runs out it will crash within months.  The orbits of the Apollo\n",
      "motherships changed noticeably during lunar missions lasting only a few\n",
      "days.  It is *possible* that there are stable orbits here and there --\n",
      "the Moon's gravitational field is poorly mapped -- but we know of none.\n",
      "\n",
      "Perturbations from Sun and Earth are relatively minor issues at low\n",
      "altitudes.  The big problem is that the Moon's own gravitational field\n",
      "is quite lumpy due to the irregular distribution of mass within the Moon. \n",
      " \n",
      "\n",
      "\n",
      "If you want to have some fun.\n",
      "\n",
      "Plug the basic formulas  into Lotus.\n",
      "\n",
      "Use the spreadsheet auto re-calc,  and graphing functions\n",
      "to produce  bar graphs  based on latitude,  tilt  and hours of day light avg.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(space_messages[0], '\\n', space_messages[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`space_messages` now contains 100 space-themed messages that we will do some feature extraction on:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Individually, please try the following:\n",
    "\n",
    "1. Use `CountVectorizer` (at its default settings) and Pandas to create a DataFrame where each column is a word in the text. \n",
    "    1. How large is your DataFrame? \n",
    "    2. What are the top 10 occuring words?\n",
    "2. Reinstantiate `CountVectorizer` and set `ngram_range` to `(1, 2)`, then fit and transform the space messages into a DataFrame\n",
    "    1. What does this do?\n",
    "    2. How large is your DataFrame?\n",
    "    3. What are the top 10 occuring words?\n",
    "3. Reinstatiate `CountVectorizer` and set `min_df` to `0.10` and `max_df` to `0.90`, then fit and transform the space messages into a DataFrame\n",
    "    1. What does this do?\n",
    "    2. How large is your DataFrame?\n",
    "    3. What are the top 10 occuring words?\n",
    "4. Reinstate `CountVectorizer` and set `max_features` to `100`, then fit and transform the space messages into a DataFrame\n",
    "    1. What does this do?\n",
    "    2. How large is your DataFrame?\n",
    "    3. What are the top 10 occuring words?\n",
    "5. Pick your smallest DataFrame and do a correlation heatmap on it. Do you see any trends? Do words co-occur together?\n",
    "\n",
    "When finished, with a partner or a small group, answer the following:\n",
    "\n",
    "6. What are each of these techniques doing? Which words are being dropped with each different technique? Which technique do you think captures the most useful words from the documents?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nAny lunar satellite needs fuel to do regular...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\nGlad to see Griffin is spending his time on ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n\\n\\nIn spite of my great respect for the peo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n\\n\\n\\n\\nDidn't one of the early jet fighters...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I just got out of the Army. Go signal corps or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\\nIf you are really interested in these orbits...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Forwarded from Neal Ausman, Galileo Mission Di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\\nAllen, sometimes I think you're OK.  And som...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\\nYou're assuming that the low-cost delivery s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\\nThis activity is regularly reported in Ron's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>\\nAlso remember that every dollar spent keepin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>How about transferring control to a non-profit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>I have two books, both NASA Special Publicatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>\\nI missed the presentations given in the morn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>\\n\\nI think you would lose your money. Julius ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>This might a real wierd idea or maybe not..\\n\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>\\n\\n\\n\\nThat would not explain why widely sepa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>\\n\\n\\nThe Command Loss Timer is a timer that d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>\\n\\n\\nIf by that you mean anything on the GD a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>\\nCould be the (folk?) song \"Clementine\".  If ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>I am sure  Mary or Henry can describe this mor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>\\nAh, there's the rub.  And a catch-22 to boot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>\\n\\n\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Sorry for asking a question that's not entirel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>zillion lines in response to article &lt;1993Apr1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>\\nIf raw materials where to cost enough that g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Reading from a Amoco Performance Products data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Archive-name: space/astronaut\\nLast-modified: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Other idea for old space crafts is as navig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>\\nWhatabout, Schools, Universities, Rich Indiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Archive-name: space/addresses\\nLast-modified: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>----- News saved at 23 Apr 93 22:22:40 GMT\\n  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>You forget that Apollo was a Government progra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>SSF is up for redesign again.  Let's do it rig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>\\n: &gt;There is an emergency oxygen system that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>\\n\\n\\nYes, and I do everyone else.  Why, you m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>\\n\\nThey recycled a lot of models and theme mu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>Why do spacecraft have to be shut off after fu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>I like option C of the new space station desig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>News-Software: UReply 3.1\\nX-X-From: Wingert@V...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>\\n\\nThey exist.  Even photosynthetic varieties...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>Hi all,\\n\\n    I'm trying to get mailing addre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>Archive-name: space/math\\nLast-modified: $Date...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>\\n\\nToo bad they didn't  give him a tour of th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>\\n\\nNever said you didn't publish, merely that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>I am coordinating the Space Shuttle Program Of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>\\nI love the idea of progressive developmental...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>Choose any or all of the following as an answe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>\\n\\tSigh.\\n\\tI try to make a little joke, I tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>\\n\\nLot's of these small miners  are no longer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>\\nare you sure 45g is the right number? as far...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>The most current orbital elements from the NOR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>(excerpts from posting on this topic) \\n\\n\\n(...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>\\nYou're assuming that \"go solar\" = \"photovolt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>\\n\\n\\nPunchline #3: it would be a good idea ju...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>on Date: 01 Apr 93 18:03:12 GMT, Ralph Buttigi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>\\nCould it be Public Missile, Inc in Michigan?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>\\nActually, the reboost will probably be done ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>\\n\\n\\nIf you want to have some fun.\\n\\nPlug th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text\n",
       "0   \\nAny lunar satellite needs fuel to do regular...\n",
       "1   \\nGlad to see Griffin is spending his time on ...\n",
       "2   \\n\\n\\nIn spite of my great respect for the peo...\n",
       "3   \\n\\n\\n\\n\\nDidn't one of the early jet fighters...\n",
       "4   I just got out of the Army. Go signal corps or...\n",
       "5   \\nIf you are really interested in these orbits...\n",
       "6   Forwarded from Neal Ausman, Galileo Mission Di...\n",
       "7   \\nAllen, sometimes I think you're OK.  And som...\n",
       "8   \\nYou're assuming that the low-cost delivery s...\n",
       "9   \\nThis activity is regularly reported in Ron's...\n",
       "10  \\nAlso remember that every dollar spent keepin...\n",
       "11  How about transferring control to a non-profit...\n",
       "12  I have two books, both NASA Special Publicatio...\n",
       "13  \\nI missed the presentations given in the morn...\n",
       "14  \\n\\nI think you would lose your money. Julius ...\n",
       "15  This might a real wierd idea or maybe not..\\n\\...\n",
       "16  \\n\\n\\n\\nThat would not explain why widely sepa...\n",
       "17  \\n\\n\\nThe Command Loss Timer is a timer that d...\n",
       "18  \\n\\n\\nIf by that you mean anything on the GD a...\n",
       "19  \\nCould be the (folk?) song \"Clementine\".  If ...\n",
       "20  I am sure  Mary or Henry can describe this mor...\n",
       "21  \\nAh, there's the rub.  And a catch-22 to boot...\n",
       "22                                           \\n\\n\\n\\n\n",
       "23  Sorry for asking a question that's not entirel...\n",
       "24  zillion lines in response to article <1993Apr1...\n",
       "25  \\nIf raw materials where to cost enough that g...\n",
       "26  Reading from a Amoco Performance Products data...\n",
       "27  Archive-name: space/astronaut\\nLast-modified: ...\n",
       "28     Other idea for old space crafts is as navig...\n",
       "29                                                   \n",
       "..                                                ...\n",
       "70  \\nWhatabout, Schools, Universities, Rich Indiv...\n",
       "71  Archive-name: space/addresses\\nLast-modified: ...\n",
       "72  ----- News saved at 23 Apr 93 22:22:40 GMT\\n  ...\n",
       "73  You forget that Apollo was a Government progra...\n",
       "74  SSF is up for redesign again.  Let's do it rig...\n",
       "75  \\n: >There is an emergency oxygen system that ...\n",
       "76  \\n\\n\\nYes, and I do everyone else.  Why, you m...\n",
       "77  \\n\\nThey recycled a lot of models and theme mu...\n",
       "78  Why do spacecraft have to be shut off after fu...\n",
       "79  I like option C of the new space station desig...\n",
       "80  News-Software: UReply 3.1\\nX-X-From: Wingert@V...\n",
       "81  \\n\\nThey exist.  Even photosynthetic varieties...\n",
       "82  Hi all,\\n\\n    I'm trying to get mailing addre...\n",
       "83  Archive-name: space/math\\nLast-modified: $Date...\n",
       "84  \\n\\nToo bad they didn't  give him a tour of th...\n",
       "85  \\n\\nNever said you didn't publish, merely that...\n",
       "86  I am coordinating the Space Shuttle Program Of...\n",
       "87  \\nI love the idea of progressive developmental...\n",
       "88  Choose any or all of the following as an answe...\n",
       "89  \\n\\tSigh.\\n\\tI try to make a little joke, I tr...\n",
       "90  \\n\\nLot's of these small miners  are no longer...\n",
       "91  \\nare you sure 45g is the right number? as far...\n",
       "92  The most current orbital elements from the NOR...\n",
       "93   (excerpts from posting on this topic) \\n\\n\\n(...\n",
       "94  \\nYou're assuming that \"go solar\" = \"photovolt...\n",
       "95  \\n\\n\\nPunchline #3: it would be a good idea ju...\n",
       "96  on Date: 01 Apr 93 18:03:12 GMT, Ralph Buttigi...\n",
       "97  \\nCould it be Public Missile, Inc in Michigan?...\n",
       "98  \\nActually, the reboost will probably be done ...\n",
       "99  \\n\\n\\nIf you want to have some fun.\\n\\nPlug th...\n",
       "\n",
       "[100 rows x 1 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame().from_dict({0: space_messages})\n",
    "df.columns = ['text']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>750</th>\n",
       "      <th>86</th>\n",
       "      <th>ago</th>\n",
       "      <th>am</th>\n",
       "      <th>an</th>\n",
       "      <th>and</th>\n",
       "      <th>any</th>\n",
       "      <th>application</th>\n",
       "      <th>...</th>\n",
       "      <th>we</th>\n",
       "      <th>week</th>\n",
       "      <th>why</th>\n",
       "      <th>will</th>\n",
       "      <th>with</th>\n",
       "      <th>would</th>\n",
       "      <th>writing</th>\n",
       "      <th>years</th>\n",
       "      <th>you</th>\n",
       "      <th>your</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>80</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 111 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    00  000  750  86  ago  am  an  and  any  application  ...   we  week  why  \\\n",
       "0    0    0    0   0    0   0   0    3    1            0  ...    1     0    0   \n",
       "1    0    0    0   0    0   0   0    0    0            0  ...    0     0    0   \n",
       "2    0    0    0   0    0   0   0    3    0            0  ...    0     0    0   \n",
       "3    0    0    0   0    0   0   0    0    0            0  ...    0     0    0   \n",
       "4    0    0    0   0    0   0   0    0    0            0  ...    0     0    0   \n",
       "5    0    0    0   0    0   0   0    4    0            0  ...    0     0    0   \n",
       "6    0    1    0   1    0   0   1   10    0            0  ...    0     0    0   \n",
       "7    0    0    0   0    0   0   0    1    0            0  ...    0     0    0   \n",
       "8    0    0    0   0    0   0   1    0    0            0  ...    0     0    2   \n",
       "9    0    0    0   0    0   0   0    0    0            0  ...    0     0    0   \n",
       "10   0    0    0   0    0   0   2    0    0            0  ...    0     0    0   \n",
       "11   0    0    0   0    0   0   0    0    0            0  ...    0     0    0   \n",
       "12   0    0    0   0    0   0   0    1    0            0  ...    0     0    1   \n",
       "13   0    0    0   0    0   0   0    4    0            0  ...    0     0    0   \n",
       "14   0    0    0   0    0   0   1    3    0            0  ...    1     0    0   \n",
       "15   0    0    0   0    0   0   0    8    1            0  ...    1     0    5   \n",
       "16   0    0    0   0    0   0   1    2    0            0  ...    0     0    1   \n",
       "17   0    0    0   0    0   0   0    1    0            0  ...    1     0    0   \n",
       "18   0    0    0   0    0   0   1    0    0            0  ...    0     1    0   \n",
       "19   0    0    0   0    0   0   0    1    0            0  ...    0     0    0   \n",
       "20   0    0    0   0    0   1   0    1    0            0  ...    0     0    0   \n",
       "21   0    0    0   0    0   0   0    3    0            0  ...    0     0    0   \n",
       "22   0    0    0   0    0   0   0    0    0            0  ...    0     0    0   \n",
       "23   0    0    0   0    0   1   1    4    0            0  ...    0     0    0   \n",
       "24   0    0    0   0    1   0   0    0    0            0  ...    0     0    0   \n",
       "25   0    0    0   0    0   0   0    0    0            0  ...    0     0    0   \n",
       "26   0    5    0   0    0   0   0    3    0            0  ...    5     0    0   \n",
       "27   0    0    0   0    0   0  15   80    2            5  ...    2     0    0   \n",
       "28   0    0    0   0    0   0   0    1    1            0  ...    0     0    1   \n",
       "29   0    0    0   0    0   0   0    0    0            0  ...    0     0    0   \n",
       "..  ..  ...  ...  ..  ...  ..  ..  ...  ...          ...  ...   ..   ...  ...   \n",
       "70   0    0    0   0    0   0   0    2    1            0  ...    1     0    0   \n",
       "71   0    1    0   0    0   0   2   26    1            0  ...    0     3    0   \n",
       "72   0    0    0   0    0   0   1    2    0            0  ...    0     0    0   \n",
       "73   0    0    0   0    1   0   0    1    0            0  ...    0     0    0   \n",
       "74   0    0    0   0    0   0   0   27    0            0  ...   11     0    0   \n",
       "75   0    0    0   0    0   0   2    1    0            0  ...    1     0    0   \n",
       "76   0    0    0   0    0   0   0    1    0            0  ...    0     0    1   \n",
       "77   0    0    0   0    0   0   0    1    0            0  ...    0     0    0   \n",
       "78   0    0    0   0    0   0   2    8    1            0  ...    2     1    3   \n",
       "79   0    0    0   0    0   0   1    3    0            0  ...    0     0    0   \n",
       "80   0    0    0   0    0   0   0    2    0            0  ...    2     0    0   \n",
       "81   0    0    0   0    0   0   0    0    0            0  ...    0     0    0   \n",
       "82   0    0    0   0    0   0   1    0    0            0  ...    0     0    0   \n",
       "83   0    4    0   0    0   0  12   50    0            0  ...    0     2    0   \n",
       "84   0    0    0   0    0   0   1    1    1            0  ...    0     0    1   \n",
       "85   0    0    0   0    0   0   0    2    0            0  ...    0     0    1   \n",
       "86   0    0    0   0    0   1   0    0    0            0  ...    0     0    0   \n",
       "87   0    0    0   0    0   0   1    1    0            0  ...    0     0    0   \n",
       "88   0    0    0   0    0   0   1    0    1            0  ...    0     0    0   \n",
       "89   0    0    0   0    0   0   0    2    0            0  ...    0     0    0   \n",
       "90   0    0    0   0    0   0   0    2    0            0  ...    0     0    0   \n",
       "91   0    0    0   0    0   0   0    0    0            0  ...    0     0    0   \n",
       "92   0    0    0   1    0   0   0    4    0            0  ...    0     0    0   \n",
       "93   0    0    0   0    0   0   5    7    2            0  ...    0     0    0   \n",
       "94   0    0    0   0    0   0   0    0    0            0  ...    0     0    0   \n",
       "95   0    0    0   0    0   0   0    0    0            0  ...    0     0    0   \n",
       "96   0    0    0   0    0   0   1    1    0            0  ...    0     0    1   \n",
       "97   0    0    0   0    0   0   3    5    2            0  ...    0     1    0   \n",
       "98   0    0    0   0    0   0   3    4    0            0  ...    0     0    0   \n",
       "99   0    0    0   0    0   0   0    2    0            0  ...    0     0    0   \n",
       "\n",
       "    will  with  would  writing  years  you  your  \n",
       "0      1     0      0        0      0    0     0  \n",
       "1      0     1      0        0      0    0     0  \n",
       "2      0     0      2        0      0    3     0  \n",
       "3      0     1      0        0      0    0     0  \n",
       "4      1     1      0        0      0    2     0  \n",
       "5      0     0      0        0      0    2     0  \n",
       "6      0     2      0        0      0    0     0  \n",
       "7      0     0      0        0      0    4     0  \n",
       "8      0     1      0        0      1    5     2  \n",
       "9      0     0      0        0      0    0     0  \n",
       "10     1     0      1        0      0    3     0  \n",
       "11     0     0      0        0      0    0     0  \n",
       "12     0     1      0        0      0    0     0  \n",
       "13     0     1      0        0      0    0     0  \n",
       "14     0     0      1        0      0    2     1  \n",
       "15     0     1      3        0      0    0     0  \n",
       "16     0     3      5        0      0    3     0  \n",
       "17     0     0      0        0      0    0     0  \n",
       "18     0     0      0        0      0    1     0  \n",
       "19     0     0      0        0      0    1     0  \n",
       "20     0     0      0        0      0    1     0  \n",
       "21     1     0      2        0      1    6     2  \n",
       "22     0     0      0        0      0    0     0  \n",
       "23     1     0      0        0      0    1     1  \n",
       "24     1     0      0        0      0    1     0  \n",
       "25     0     0      2        0      0    0     0  \n",
       "26     8     4      0        0      0    6     0  \n",
       "27    29     7      1        1      4   19     5  \n",
       "28     0     1      0        0      0    5     0  \n",
       "29     0     0      0        0      0    0     0  \n",
       "..   ...   ...    ...      ...    ...  ...   ...  \n",
       "70     0     1      0        0      0    0     0  \n",
       "71     2     4      0        0      0    4     1  \n",
       "72     0     0      0        0      0    0     0  \n",
       "73     0     0      0        0      0    1     0  \n",
       "74     1     3      0        0      0    0     0  \n",
       "75     0     3      2        0      0    1     0  \n",
       "76     0     0      1        0      0    1     0  \n",
       "77     0     0      0        0      0    0     0  \n",
       "78     0     0      2        0      1    0     0  \n",
       "79     0     0      0        0      0    0     0  \n",
       "80     0     0      0        0      0    1     1  \n",
       "81     0     0      0        0      0    0     0  \n",
       "82     1     0      1        0      0    0     0  \n",
       "83     1     9      2        0      2   13     3  \n",
       "84     0     1      1        0      0    0     0  \n",
       "85     0     1      0        0      1    4     0  \n",
       "86     0     0      0        0      0    1     0  \n",
       "87     0     0      0        0      0    0     0  \n",
       "88     0     0      0        0      0    0     0  \n",
       "89     1     0      1        0      0    0     0  \n",
       "90     0     0      0        0      0    1     0  \n",
       "91     0     0      1        0      0    1     0  \n",
       "92     0     0      0        0      0    0     0  \n",
       "93     1     2      2        0      0    0     0  \n",
       "94     0     0      0        0      0    3     0  \n",
       "95     0     0      1        0      0    0     0  \n",
       "96     0     0      0        0      0    0     0  \n",
       "97     0     1      0        0      0    1     0  \n",
       "98     1     0      0        0      0    3     0  \n",
       "99     0     0      0        0      0    1     0  \n",
       "\n",
       "[100 rows x 111 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(cv.transform(df['text']).todense(), \n",
    "             columns=cv.get_feature_names()) # use .get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certain words occur a huge amount but may not offer anything useful to us as modelers. We'll discuss removing these words shortly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `HashingVectorizer`\n",
    "\n",
    "Sometimes we may have too many features or too many rows and run out of memory trying to process all of it. Sklearn provides a second option known as `HashingVectorizer` to avoid that.\n",
    "\n",
    "- Benefit: Low memory footprint, allowing you to do more work\n",
    "- Downside: No ability to determine which feature corresponds to what the original word is.\n",
    "\n",
    "In most cases, we'll be using `CountVectorizer`, but `HashingVectorizer` can be a nice ace in the hole. \n",
    "\n",
    "#### How it Works\n",
    "\n",
    "As you have seen we can set the `CountVectorizer` dictionary to have a fixed size, only keeping words of certain frequencies, however, we still have to compute a dictionary and hold the dictionary in memory. This could be a problem when we have a large corpus or in streaming applications where we don't know which words we will encounter in the future.\n",
    "\n",
    "These problems can be solved using the `HashingVectorizer`, which converts a collection of text documents to a matrix of occurrences, calculated with the hashing trick. Each word is mapped to a feature with the use of a hash function that converts it to a hash. If we encounter that word again in the text, it will be converted to the same hash, allowing us to count word occurence without retaining a dictionary in memory. This is very convenient!\n",
    "\n",
    "The main drawback of the this trick is that it's not possible to compute the inverse transform, and thus we lose information on what words the important features correspond to. The hash function employed is the signed 32-bit version of Murmurhash3.\n",
    "\n",
    "We'll instantiate `HashingVectorizer` with the keyword argument `norm=None` to get back real value counts of words. The default is to normalize these values so as to prevent hashing collisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "479532    461.0\n",
       "174171    455.0\n",
       "828689    302.0\n",
       "144749    229.0\n",
       "329790    177.0\n",
       "832412    143.0\n",
       "512176    122.0\n",
       "170062    119.0\n",
       "994433    103.0\n",
       "408423    100.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hvec = HashingVectorizer(norm=None)\n",
    "hvec.fit(space_messages)\n",
    "hvec.n_features\n",
    "\n",
    "answer = pd.DataFrame(hvec.transform(space_messages).todense())\n",
    "answer.sum().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often, slightly different version of a word exist. For example: LinkedIn sees 6000+ variations of the title \"Software Engineer\" and 8000+ variations of the word \"IBM\".\n",
    "\n",
    "It would be wrong to consider the words \"MR.\" and \"mr\" to be different features, thus we need a technique to normalize words to a common root. This technique is called Stemming.\n",
    "\n",
    "- Science, Scientist => Scien\n",
    "- Swimming, Swimmer, Swim => Swim\n",
    "\n",
    "We could define a Stemmer based on rules that we've decided on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Scien', 'Scien']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stem(tokens):\n",
    "    '''rules-based stemming of a bunch of tokens'''\n",
    "    \n",
    "    new_bag = []\n",
    "    for token in tokens:\n",
    "        # define rules here\n",
    "        if token.endswith('s'):\n",
    "            new_bag.append(token[:-1])\n",
    "        elif token.endswith('er'):\n",
    "            new_bag.append(token[:-2])\n",
    "        elif token.endswith('tion'):\n",
    "            new_bag.append(token[:-4])\n",
    "        elif token.endswith('tist'):\n",
    "            new_bag.append(token[:-4])\n",
    "        elif token.endswith('ce'):\n",
    "            new_bag.append(token[:-2])\n",
    "        elif token.endswith('ing'):\n",
    "            new_bag.append(token[:-2])\n",
    "        else:\n",
    "            new_bag.append(token)\n",
    "\n",
    "    return new_bag\n",
    "\n",
    "stem(['Science', 'Scientist'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But it is often easier to make use of `nltk` to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scientist\n",
      "scientist\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "print(stemmer.stem('scientists'))\n",
    "print(stemmer.stem('scientist'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Some words are very common and provide no information on the text content.\n",
    "\n",
    "We should remove these stop words. Every language has different stop words and you can add your own words as well if it makes sense (for example, if you there was a domain word that was prevelant but ultimately uninformative, you should use your judgement to remove it).\n",
    "\n",
    "`nltk` provides their own list of stop words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource 'corpora/stopwords' not found.  Please use the NLTK\n  Downloader to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - '/Users/yuesu/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m//anaconda/envs/py36/lib/python3.6/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py36/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    652\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 653\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource 'corpora/stopwords.zip/stopwords/' not found.  Please\n  use the NLTK Downloader to obtain the resource:  >>>\n  nltk.download()\n  Searched in:\n    - '/Users/yuesu/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-78c080faef60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mstop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py36/lib/python3.6/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py36/lib/python3.6/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py36/lib/python3.6/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py36/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    651\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 653\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource 'corpora/stopwords' not found.  Please use the NLTK\n  Downloader to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - '/Users/yuesu/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "print(stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use `CountVectorizer` to remove common stopwords for us by setting the `stop_words` keyword argument to `english`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the     989\n",
      "of      461\n",
      "to      455\n",
      "and     453\n",
      "in      302\n",
      "for     237\n",
      "is      229\n",
      "it      177\n",
      "that    167\n",
      "be      149\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer()\n",
    "cv.fit(space_messages)\n",
    "stopped_words = pd.DataFrame(cv.transform(space_messages).todense(),\n",
    "                              columns=cv.get_feature_names())\n",
    "print(stopped_words.sum().sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "space         144\n",
      "nasa           79\n",
      "earth          53\n",
      "shuttle        39\n",
      "spacecraft     37\n",
      "program        37\n",
      "time           36\n",
      "launch         36\n",
      "mission        35\n",
      "flight         34\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(stop_words='english')\n",
    "cv.fit(space_messages)\n",
    "stopped_words = pd.DataFrame(cv.transform(space_messages).todense(),\n",
    "                              columns=cv.get_feature_names())\n",
    "print(stopped_words.sum().sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dramatically changes the type of \"common\" words available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to combine transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with text data requires a lot of preprocessing, as the effectiveness of your model can dramatically change based on how well you've processed your text data. \n",
    "\n",
    "In **most** cases, tweaking the settings of `CountVectorizer` will generally be ok. However, sometimes you may want to create a custom function to preprocess text for `CountVectorizer`.\n",
    "\n",
    "Imagine that I wanted to do the following to my text:\n",
    "\n",
    "- Remove punctuation and numbers\n",
    "- Turn text lowercase\n",
    "- Remove stop words\n",
    "- Stem remaining words\n",
    "\n",
    "We could imagine building a function to do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaner(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    stop = stopwords.words('english')\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = text.translate(str.maketrans('', '', string.digits))\n",
    "    text = text.lower().strip()\n",
    "    final_text = []\n",
    "    for w in text.split():\n",
    "        if w not in stop:\n",
    "            final_text.append(stemmer.stem(w.strip()))\n",
    "    return ' '.join(final_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That would change our original spam message from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hello, I saw your contact information on LinkedIn. I have carefully read through your profile and you seem to have an outstanding personality. This is one major reason why I am in contact with you. My name is Mr. Valery Grayfer Chairman of the Board of Directors of PJSC \"LUKOIL\". I am 86 years old and I was diagnosed with cancer 2 years ago. I will be going in for an operation later this week. I decided to WILL/Donate the sum of 8,750,000.00 Euros(Eight Million Seven Hundred And Fifty Thousand Euros Only etc. etc.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource 'corpora/stopwords' not found.  Please use the NLTK\n  Downloader to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - '/Users/yuesu/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m//anaconda/envs/py36/lib/python3.6/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py36/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    652\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 653\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource 'corpora/stopwords.zip/stopwords/' not found.  Please\n  use the NLTK Downloader to obtain the resource:  >>>\n  nltk.download()\n  Searched in:\n    - '/Users/yuesu/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-e37f1281654e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspam_cleaned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcleaner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspam_cleaned\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-2a0354e2eb0d>\u001b[0m in \u001b[0;36mcleaner\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcleaner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mstemmer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mstop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaketrans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaketrans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdigits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py36/lib/python3.6/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py36/lib/python3.6/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py36/lib/python3.6/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py36/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    651\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 653\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource 'corpora/stopwords' not found.  Please use the NLTK\n  Downloader to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - '/Users/yuesu/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************"
     ]
    }
   ],
   "source": [
    "spam_cleaned = cleaner(spam)\n",
    "print(spam_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can pass this function into `CountVectorizer` as a way to preprocess the text as a part of the fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource 'corpora/stopwords' not found.  Please use the NLTK\n  Downloader to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - '/Users/yuesu/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m//anaconda/envs/py36/lib/python3.6/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py36/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    652\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 653\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource 'corpora/stopwords.zip/stopwords/' not found.  Please\n  use the NLTK Downloader to obtain the resource:  >>>\n  nltk.download()\n  Searched in:\n    - '/Users/yuesu/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-a83fe3606333>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcleaner\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspace_messages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m custom_preprocess = pd.DataFrame(cv.transform(space_messages).todense(),\n\u001b[1;32m      4\u001b[0m                               columns=cv.get_feature_names())\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustom_preprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py36/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    804\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m         \"\"\"\n\u001b[0;32m--> 806\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    807\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py36/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m--> 839\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py36/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    760\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py36/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[0;32m--> 241\u001b[0;31m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-2a0354e2eb0d>\u001b[0m in \u001b[0;36mcleaner\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcleaner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mstemmer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mstop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaketrans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaketrans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdigits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py36/lib/python3.6/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py36/lib/python3.6/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py36/lib/python3.6/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py36/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    651\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 653\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource 'corpora/stopwords' not found.  Please use the NLTK\n  Downloader to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - '/Users/yuesu/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(preprocessor=cleaner)\n",
    "cv.fit(space_messages)\n",
    "custom_preprocess = pd.DataFrame(cv.transform(space_messages).todense(),\n",
    "                              columns=cv.get_feature_names())\n",
    "print(custom_preprocess.sum().sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to use a custom preprocessor callable\n",
    "\n",
    "Starting off, I'd make use of what's built into `CountVectorizer` and see if that changes your outcome. However, if you've been working on a dataset for a longer amount of time (such as during your projects or your Capstone) you may want to start developing custom preprocessors that make more sense for the domain that you are in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term frequency - Inverse document Frequency (tf-idf)\n",
    "\n",
    "Tf-idf tells us which words are the most discriminating between documents. Words that occur a lot in one document but don't occur in many other documents tells you a lot more than a word that occurs frequently in all documents.\n",
    "\n",
    "Take a look at the map below. These types of maps usually highlight the most **uniquely favorite food** of a state, not the actual favorite food (which in most cases is probably pizza).\n",
    "\n",
    "Tf-idf, conceptually, does the same thing: it highlights what is common or typical in one or two cases and rare in all others. That's usually more interesting (and predictive) then words or items that are common everywhere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/map-foods.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Term frequency is the frequency of a certain term in a document\n",
    "- Inverse document frequency is defined as the frequency of documents that contain that term over the whole corpus.\n",
    "\n",
    "This technique reweights terms to strengthen those that are highly specific to a particular document, while suppressing terms that are common to most documents.\n",
    "\n",
    "Formally, you'll usually see it like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ tf(t, d) = \\frac{N_{term}}{N_{document}} $$\n",
    "\n",
    "$$ idf(t, D) = log(\\frac{N_{documents}}{N_{\\text{documents that contain term t}}}) $$\n",
    "\n",
    "We'll use sklearn's `TfidfVectorizer` to count vectorize and then transform that data using Tfidf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>750</th>\n",
       "      <th>86</th>\n",
       "      <th>ago</th>\n",
       "      <th>application</th>\n",
       "      <th>assistance</th>\n",
       "      <th>attached</th>\n",
       "      <th>best</th>\n",
       "      <th>board</th>\n",
       "      <th>...</th>\n",
       "      <th>seven</th>\n",
       "      <th>site</th>\n",
       "      <th>smith</th>\n",
       "      <th>sum</th>\n",
       "      <th>thousand</th>\n",
       "      <th>time</th>\n",
       "      <th>valery</th>\n",
       "      <th>week</th>\n",
       "      <th>writing</th>\n",
       "      <th>years</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>0.145067</td>\n",
       "      <td>0.145067</td>\n",
       "      <td>0.145067</td>\n",
       "      <td>0.145067</td>\n",
       "      <td>0.145067</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.145067</td>\n",
       "      <td>...</td>\n",
       "      <td>0.145067</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.145067</td>\n",
       "      <td>0.145067</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.145067</td>\n",
       "      <td>0.145067</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.290133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.155195</td>\n",
       "      <td>0.155195</td>\n",
       "      <td>0.155195</td>\n",
       "      <td>0.155195</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.155195</td>\n",
       "      <td>0.155195</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.155195</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.155195</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 68 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            00       000       750        86       ago  application  \\\n",
       "spam  0.145067  0.145067  0.145067  0.145067  0.145067     0.000000   \n",
       "ham   0.000000  0.000000  0.000000  0.000000  0.000000     0.155195   \n",
       "\n",
       "      assistance  attached      best     board    ...        seven      site  \\\n",
       "spam    0.000000  0.000000  0.000000  0.145067    ...     0.145067  0.000000   \n",
       "ham     0.155195  0.155195  0.155195  0.000000    ...     0.000000  0.155195   \n",
       "\n",
       "         smith       sum  thousand      time    valery      week   writing  \\\n",
       "spam  0.000000  0.145067  0.145067  0.000000  0.145067  0.145067  0.000000   \n",
       "ham   0.155195  0.000000  0.000000  0.155195  0.000000  0.000000  0.155195   \n",
       "\n",
       "         years  \n",
       "spam  0.290133  \n",
       "ham   0.000000  \n",
       "\n",
       "[2 rows x 68 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tvec = TfidfVectorizer(stop_words='english')\n",
    "tvec.fit([spam, ham])\n",
    "\n",
    "df = pd.DataFrame(tvec.transform([spam, ham]).todense(),\n",
    "                   columns=tvec.get_feature_names(),\n",
    "                   index=['spam', 'ham'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we see that:\n",
    "\n",
    "- words that don't show up a document have `0.0000` for values\n",
    "- words that do show up in a document are weighted based on tf-idf -- as those scores increase in value, the those terms occur more uniquely in document _d_ than in other documents. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for Understanding 3 (15 minutes)\n",
    "\n",
    "Let's practice some of these new techniques on `space_messages` that we established previously. As a reminder, these contain the first 100 messages in the space newsgroups dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Any lunar satellite needs fuel to do regular orbit corrections, and when\n",
      "its fuel runs out it will crash within months.  The orbits of the Apollo\n",
      "motherships changed noticeably during lunar missions lasting only a few\n",
      "days.  It is *possible* that there are stable orbits here and there --\n",
      "the Moon's gravitational field is poorly mapped -- but we know of none.\n",
      "\n",
      "Perturbations from Sun and Earth are relatively minor issues at low\n",
      "altitudes.  The big problem is that the Moon's own gravitational field\n",
      "is quite lumpy due to the irregular distribution of mass within the Moon.\n"
     ]
    }
   ],
   "source": [
    "print(space_messages[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Individually, please try and tackle the following questions. You may find it helpful to use some of the code you wrote for the first Check for Understanding.\n",
    "\n",
    "1. Use the default `CountVectorizer()` to create a DataFrame and answer the following questions (note, we also did this same prompt in the first CfU -- this is to remind us of where we're starting from):\n",
    "    - How large is your DataFrame?\n",
    "    - What are the top 10 occuring words?\n",
    "2. Use `CountVectorizer()` and remove stop words with the `stop_words` keyword argument. \n",
    "    - How large is your DataFrame?\n",
    "    - What are the top 10 occuring words?\n",
    "3. Use `TfIdfVectorizer()` and remove stop words with the `stop_words` keyword argument.\n",
    "    - How large is your DataFrame?\n",
    "    - Interpret the values that are in the DataFrame. What do they mean?\n",
    "4. We've tried a bunch of different ways to transform the same text data into numbers across this check for understanding and the previous one. Pick two methods and compare and contrast them. What type of words are included in each DataFrame? What types of words are removed or reduced in impact?\n",
    "\n",
    "When you're finished working individually, share your work with a partner. Did you come to the same conclusions? What happens if you try this with new data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to apply NLP\n",
    "\n",
    "Typically, we'll use NLP in the feature engineering stage to help us create new features for our data, which we'll then use in either structured or unstructured modeling techniques. What follows is an example of how we might apply this process. We'll first do this in an ad-hoc fashion, then refactor our code for a more production-ready format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a truncated dataset about the news. This is a set of 8000 randomly selected news articles with an indicator as to whether the news article is discussing economic news or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relevance</th>\n",
       "      <th>headline</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>yes</td>\n",
       "      <td>Yields on CDs Fell in the Latest Week</td>\n",
       "      <td>NEW YORK -- Yields on most certificates of dep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>no</td>\n",
       "      <td>The Morning Brief: White House Seeks to Limit ...</td>\n",
       "      <td>The Wall Street Journal Online&lt;/br&gt;&lt;/br&gt;The Mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>no</td>\n",
       "      <td>Banking Bill Negotiators Set Compromise --- Pl...</td>\n",
       "      <td>WASHINGTON -- In an effort to achieve banking ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>no</td>\n",
       "      <td>Manager's Journal: Sniffing Out Drug Abusers I...</td>\n",
       "      <td>The statistics on the enormous costs of employ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yes</td>\n",
       "      <td>Currency Trading: Dollar Remains in Tight Rang...</td>\n",
       "      <td>NEW YORK -- Indecision marked the dollar's ton...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  relevance                                           headline  \\\n",
       "0       yes              Yields on CDs Fell in the Latest Week   \n",
       "1        no  The Morning Brief: White House Seeks to Limit ...   \n",
       "2        no  Banking Bill Negotiators Set Compromise --- Pl...   \n",
       "3        no  Manager's Journal: Sniffing Out Drug Abusers I...   \n",
       "4       yes  Currency Trading: Dollar Remains in Tight Rang...   \n",
       "\n",
       "                                                text  \n",
       "0  NEW YORK -- Yields on most certificates of dep...  \n",
       "1  The Wall Street Journal Online</br></br>The Mo...  \n",
       "2  WASHINGTON -- In an effort to achieve banking ...  \n",
       "3  The statistics on the enormous costs of employ...  \n",
       "4  NEW YORK -- Indecision marked the dollar's ton...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "econ = pd.read_csv('datasets/economic_news.csv', \n",
    "                   usecols=[7, 11, 14])\n",
    "econ.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEW YORK -- Yields on most certificates of deposit offered by major banks dropped more than a tenth of a percentage point in the latest week, reflecting the overall decline in short-term interest rates.</br></br>On small-denomination, or \"consumer,\" CDs sold directly by banks, the average yield on six-month deposits fell to 5.49% from 5.62% in the week ended yesterday, according to an 18-bank survey by Banxquote Money Markets, a Wilmington, Del., information service.</br></br>On three-month \"consumer\" deposits, the average yield sank to 5.29% from 5.42% the week before, according to Banxquote. Two banks in the Banxquote survey, Citibank in New York and CoreStates in Pennsylvania, are paying less than 5% on threemonth small-denomination CDs.</br></br>Declines were somewhat smaller on five-year consumer CDs, which eased to 7.37% from 7.45%, Banxquote said.</br></br>Yields on three-month and six-month Treasury bills sold at Monday's auction plummeted more than a fifth of a percentage point from the previous week, to 5.46% and 5.63%, respectively.\n"
     ]
    }
   ],
   "source": [
    "print(econ.iloc[0, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's dummify the `relevance` column and strip out `</br>` from our text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "econ['relevance'] = econ['relevance'].apply(lambda x: 1 if x == 'yes' else 0)\n",
    "econ['text'] = econ['text'].apply(lambda x: x.replace('</br>', ''))\n",
    "econ['headline'] = econ['headline'].apply(lambda x: x.replace('</br>', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Yields on CDs Fell in the Latest Week\n",
      "NEW YORK -- Yields on most certificates of deposit offered by major banks dropped more than a tenth of a percentage point in the latest week, reflecting the overall decline in short-term interest rates.On small-denomination, or \"consumer,\" CDs sold directly by banks, the average yield on six-month deposits fell to 5.49% from 5.62% in the week ended yesterday, according to an 18-bank survey by Banxquote Money Markets, a Wilmington, Del., information service.On three-month \"consumer\" deposits, the average yield sank to 5.29% from 5.42% the week before, according to Banxquote. Two banks in the Banxquote survey, Citibank in New York and CoreStates in Pennsylvania, are paying less than 5% on threemonth small-denomination CDs.Declines were somewhat smaller on five-year consumer CDs, which eased to 7.37% from 7.45%, Banxquote said.Yields on three-month and six-month Treasury bills sold at Monday's auction plummeted more than a fifth of a percentage point from the previous week, to 5.46% and 5.63%, respectively.\n"
     ]
    }
   ],
   "source": [
    "for x in range(3):\n",
    "    print(econ.iloc[0,x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's split into a training set and a test set, using the `text` of the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(econ['text'].values, \n",
    "                                                   econ['relevance'].values,\n",
    "                                                   test_size=0.33,\n",
    "                                                   random_state=2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforming data using `CountVectorizer`\n",
    "\n",
    "We'll use `CountVectorizer` in its default form to prepare a dataframe for modeling. Note that I'm going to leave this in its sparse form, because it typically won't matter for the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5360, 38310)\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer()\n",
    "cv.fit(X_train)\n",
    "X = cv.transform(X_train)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimensionality Reduction with PCA\n",
    "\n",
    "Typically, those 38,000+ columns are not individually informative. Reducing them in dimensionality using PCA can be very helpful. We'll be using a variant of PCA known as [`TruncatedSVD`](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html) -- it does more or less the same thing as PCA but in a slightly different fashion. It's best used with sparse matrices like these. \n",
    "\n",
    "**Note**: `TruncatedSVD` works on the actual matrix itself, _not_ the covariance matrix like PCA. This means that the maximum number of components you can make is limited to the smaller of your rows or columns. In this case, even though we have a large number of columns, we only have around 5,000 rows, meaning that the largest number of components `TruncatedSVD` will be able to make is that number.\n",
    "\n",
    "First, we'll pick a high number of components and graph the explained variance ratio to see if there's a useful cutoff point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1160ca208>]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAD3CAYAAAAALt/WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niMv9f0Favysg6ewQAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x14db11748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tsvd = TruncatedSVD(n_components=1000)\n",
    "tsvd.fit(X)\n",
    "plt.plot(range(1000), tsvd.explained_variance_ratio_.cumsum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph would suggest that somewhere north of 100 components might be pretty good (it is where the rate of change begins to flatten out). I'm going to stick with a smaller number (10) for the time being for modeling speed. This will be one of the things we tweak in our Check for Understanding later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsvd = TruncatedSVD(n_components=10)\n",
    "tsvd.fit(X)\n",
    "X_tsvd = tsvd.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting with `RandomForestClassifier`\n",
    "\n",
    "At this point, we can move forward with modeling on our new sparse matrix of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.976865671642\n",
      "[[4426    0]\n",
      " [ 124  810]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      1.00      0.99      4426\n",
      "          1       1.00      0.87      0.93       934\n",
      "\n",
      "avg / total       0.98      0.98      0.98      5360\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(X_tsvd, y_train)\n",
    "print(rfc.score(X_tsvd, y_train))\n",
    "print(confusion_matrix(y_train, rfc.predict(X_tsvd)))\n",
    "print(classification_report(y_train, rfc.predict(X_tsvd)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an exceptionally good model with high accuracy, though we do misclassify about 12% of our class 1 (economic) news as non-economic news. However, we should check to make sure that it works equally well on our test data. As before, we'll make sure that we are applying the already fit transformation to our test data, not refitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.805681818182\n",
      "[[2106   48]\n",
      " [ 465   21]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      0.98      0.89      2154\n",
      "          1       0.30      0.04      0.08       486\n",
      "\n",
      "avg / total       0.72      0.81      0.74      2640\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_test_cv = cv.transform(X_test)\n",
    "X_test_svd = tsvd.transform(X_test_cv)\n",
    "print(rfc.score(X_test_svd, y_test))\n",
    "print(confusion_matrix(y_test, rfc.predict(X_test_svd)))\n",
    "print(classification_report(y_test, rfc.predict(X_test_svd)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do a little worse here, as expected -- we almost completely misclassify all of our economic news as non-economic news. However, this process does identify our general process for modeling with text data:\n",
    "\n",
    "1. Use the NLP techniques that we've identified to clean and prepare the data\n",
    "2. Consider using a dimensionality reduction technique like `TruncatedSVD`\n",
    "3. Use the prepared data with machine learning techniques that we've already seen before to predict things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for Understanding 3 (20 minutes)\n",
    "\n",
    "This is an open ended check for understanding. Your goal at the end of 20 minutes is to have a better model with this same data that maximizes the class 1 recall of our model on the test data:\n",
    "\n",
    "> Our current model at this point correctly predicts 24 of 486 economic articles correctly -- we want to increase the predictive power of our model for that test case!\n",
    "\n",
    "How you do so is up to you (exploring how the data is transformed is highly encouraged). Some options:\n",
    "\n",
    "- Use stop words and other arguments in `CountVectorizer` to change which features are engineered\n",
    "- Make use of `TfidfVectorizer` to highlight more unique words\n",
    "- Modify your the number of components created in `TruncatedSVD`\n",
    "- Try a different modeling technique\n",
    "- Use `GridSearchCV` to optimize hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Revisiting Pipelines\n",
    "\n",
    "We may also want to create a `Pipeline` object to reproducibly and reliably transform and predict data with one step versus many. We'll refactor our code here to take advantage of that and end with a check for understanding where you can practice the same. \n",
    "\n",
    "The first step is to break down our current model into a set of sequential steps. From my original example using the economic news dataset, my steps were:\n",
    "\n",
    "1. Apply `CountVectorizer` to `X` (matrix of article text)\n",
    "2. Apply `TruncatedSVD` with 10 components to the count vectorized data\n",
    "3. Predict using `RandomForestClassifier`\n",
    "\n",
    "These steps are _sequential_ and can feed one directly into the other. We do not need to include a feature union here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = make_pipeline(\n",
    "    CountVectorizer(),\n",
    "    TruncatedSVD(n_components=10),\n",
    "    RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we had more hyperparameters or different steps, we could change what is inside that pipeline to account for that. \n",
    "\n",
    "Next, let's fit this to our training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('countvectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "  ...imators=10, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we'll score it and run the predictions from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.976305970149\n",
      "[[4425    1]\n",
      " [ 126  808]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      1.00      0.99      4426\n",
      "          1       1.00      0.87      0.93       934\n",
      "\n",
      "avg / total       0.98      0.98      0.98      5360\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(pipeline.score(X_train, y_train))\n",
    "predictions = pipeline.predict(X_train)\n",
    "print(confusion_matrix(y_train, predictions))\n",
    "print(classification_report(y_train, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, we'll score and predict with our test set. We do not need to refit this to the test set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.813257575758\n",
      "[[2124   30]\n",
      " [ 463   23]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      0.99      0.90      2154\n",
      "          1       0.43      0.05      0.09       486\n",
      "\n",
      "avg / total       0.75      0.81      0.75      2640\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(pipeline.score(X_test, y_test))\n",
    "predictions = pipeline.predict(X_test)\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check For Understanding 4 (15 Minutes)\n",
    "\n",
    "Pair up and do the following:\n",
    "\n",
    "1. Using the model you created in the last Check for Understanding, diagram out the steps involved in going from the training data to a fitted model with your partner. Highlight what steps can happen sequentially and which must happen in parallel (typically parallel steps would happen during the feature engineering stage)\n",
    "\n",
    "2. Pick a model and refactor the code to use a `Pipeline` instead of its current version. Write this code on the machine that doesn't have the original model on it. This is to help you think about what your code is *doing* versus how you've *written* it\n",
    "\n",
    "3. If you have time, replicate the process for the other model. \n",
    "\n",
    "If you used a grid search, feel free to set the best parameters manually (such as `RandomForestClassifier(n_estimators=100)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [py36]",
   "language": "python",
   "name": "Python [py36]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
